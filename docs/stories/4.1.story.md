# Story 4.1: Seed Benchmark Protocol Data

**Status:** Done

## Goal & Context

**User Story:** As a Developer, I want a script (`tools/seed_benchmark_data.py`) so that I can use pre-defined PICO text elements and a pre-defined comprehensive exclusion criteria list to create/update a corresponding `SystematicReview` record in the database. PICO elements will be stored as plain text in `criteria_framework_answers`, and an XML-formatted string from these PICO elements will populate the `inclusion_criteria` field (for LLM prompts), while the full `exclusion_criteria` text will be stored as a plain text list.

**Context:** This story is the first step in setting up the SRA Benchmarking Module as defined in "Epic 4: SRA Benchmarking Module Implementation" ([`docs/epics/epic4-sra-benchmarking-module.md`](/docs/epics/epic4-sra-benchmarking-module.md)). It establishes the foundational protocol data in the database, structured to directly feed the existing LLM screening prompts. This directly supports PRD FR1.1 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)) with specific data formatting for prompt compatibility, using pre-defined text for clarity and efficiency.

## Detailed Requirements

-   The script (`tools/seed_benchmark_data.py`) will use the **explicitly pre-defined text segments for PICO elements** (Population, Intervention/Exposure, Comparison, Outcome) as specified in the "Data Structures" section of this story.
-   These PICO text segments MUST be stored as **plain text strings** under distinct, descriptive, `snake_case` keys (e.g., `"population"`, `"intervention"`) in the `criteria_framework_answers` JSONB field.
-   The `SystematicReview.inclusion_criteria` TEXT field (used for LLM prompts) MUST be populated by:
    1. Taking the plain text PICO values (provided in this story under "Data Structures") which are stored in `criteria_framework_answers`.
    2. Programmatically wrapping each PICO element's text in distinct, `snake_case` XML tags that correspond to the keys used in `criteria_framework_answers` (e.g., `<population>Population text</population>`).
    3. Concatenating these XML-tagged strings, delimited by newline characters (`\n`).
-   The `SystematicReview.exclusion_criteria` TEXT field MUST be populated using the **explicitly pre-defined comprehensive list of exclusion criteria** provided in the "Data Structures" section of this story. This list should be stored as a single newline-delimited plain text string.
-   The script must create a new `SystematicReview` record in the database or update an existing one if a benchmark protocol record already exists (idempotency).
-   The script should clearly indicate success or failure and log its actions.

## Acceptance Criteria (ACs)

- AC1: Script uses the **explicitly provided PICO text segments** (from this story's "Data Structures" section) and stores them as plain text strings under `snake_case` keys (e.g., `population`) in the `SystematicReview.criteria_framework_answers` JSONB field.
- AC2: Script correctly constructs an **XML-formatted, newline-delimited string** from these PICO text values (e.g., `<population>Population text</population>\n<intervention>Intervention text</intervention>`) and stores this string in the `SystematicReview.inclusion_criteria` field. The `snake_case` XML tags MUST correspond to the keys used in `criteria_framework_answers`.
- AC3: Script uses the **explicitly provided comprehensive exclusion criteria list** (from this story's "Data Structures" section) and stores it as a newline-delimited **plain text string** in `SystematicReview.exclusion_criteria`.
- AC4: Script is idempotent for protocol seeding (e.g., checks for an existing benchmark protocol record using a known identifier or specific field values before creating a new one, or updates the existing one).
- AC5: The script outputs clear success or failure messages and logs its main actions (e.g., "Benchmark protocol found and updated", "New benchmark protocol created").

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Create: `tools/seed_benchmark_data.py`
    - Files to Interact With (Models/Schemas): `src/sr_assistant/core/models.py` (specifically `SystematicReview`), `src/sr_assistant/core/schemas.py` (specifically `SystematicReviewCreate`, `SystematicReviewUpdate`).
    - Potentially `src/sr_assistant/app/services.py` (if using `ReviewService` to interact with the database).
    - File to Read From: `docs/benchmark/bechmark-protocol.md` (Only for I, C, O PICO elements and any general inclusion points if a separate field for full plain text inclusions is decided later. For this story, PICO text is directly provided, and exclusion text is directly provided).

-   **Key Technologies:**
    - Python 3.12
    - SQLModel/SQLAlchemy (for DB interaction, likely via a service or repository)
    - Pydantic (for `SystematicReviewCreate`/`Update` schemas)
    - Loguru (for logging)

-   **API Interactions / SDK Usage:**
    - Primarily database interactions to create/update `SystematicReview` records.

-   **UI/UX Notes:**
    - Not applicable for this script-based story.

-   **Data Structures:**
    -   `SystematicReview` SQLModel from `src/sr_assistant/core/models.py`.
    -   `SystematicReviewCreate` and `SystematicReviewUpdate` Pydantic schemas from `src/sr_assistant/core/schemas.py`.
    -   **PICO Elements for `criteria_framework_answers` (JSONB):** The script will use the following exact plain text strings for the PICO components, storing them under the specified `snake_case` keys in the `criteria_framework_answers` JSONB field:
        - `"population"`: "Individuals experiencing homelessness. Studies must include data collected in the Republic of Ireland. The review focuses on the health of homeless individuals themselves, not on reports from key informants about their needs. Studies that only include international/European datasets without specific outcomes for the Republic of Ireland should be excluded."
        - `"intervention"`: "The review focuses on studies that generate empirical data (quantitative or qualitative) on the following health-related topics for the homeless population:\n- Overall health status.\n- Health care access, utilisation, and quality.\n- Specific health conditions (e.g., addiction, diabetes, cancer, communicable/non-communicable diseases, STIs, pregnancy and childbirth).\n- Health behaviours (e.g., nutrition, child development, tobacco use, vaccination).\n- Social determinants of health (e.g., social and community context, education, economic stability)."
        - `"comparison"`: "Studies that include a comparison/control group comprising the general, housed population are of interest.\nSuch studies should contain a method for comparing health indicator(s) between the homeless (exposed) group and the general housed (control) group (e.g., using relative risk, absolute difference, slope/relative index of inequality)."
        - `"outcome"`: "- Empirical indicators of health status.\n- Empirical indicators of health care access.\n- Empirical indicators of health care quality.\n- Empirical indicators of health care utilisation."
    -   **`SystematicReview.inclusion_criteria` Field Content (XML-formatted, newline-delimited string):** This field will be constructed by the script from the plain text values defined above for `criteria_framework_answers`. Example structure using `snake_case` tags:
    ```xml
    <population>{text for population specified above}</population>
    <intervention>{text for intervention specified above}</intervention>
    <comparison>{text for comparison specified above}</comparison>
    <outcome>{text for outcome specified above}</outcome>
    ```
    - **`SystematicReview.exclusion_criteria` Field Content (Plain Text, Newline-Delimited):** This field will be populated with the following exact newline-delimited string:
    ```text

Population-related:

-   Studies focusing on key informants reporting on the needs of the homeless, rather than the homeless population directly.
-   Studies with no data from the Republic of Ireland.
-   Studies using international/European datasets that include data from Ireland but do not report outcomes specific to the Republic of Ireland.
Study Design & Publication Type-related:
-   Studies that do not generate empirical primary or secondary data on a health topic. This includes:
    - Modelling studies.
    - Commentaries/Letters.
    - Individual case reports.
-   Conference Abstracts.
-   Policy papers.
-   Guidelines.
-   Grey literature (e.g., government documents and reports, pre-print articles, research reports, statistical reports) - due to resource limitations for a thorough search.
Topic-related:
-   Animal studies.
-   Studies on economic, health care, or housing policy that do not relate directly to health outcomes or access for the homeless population.
Language-related:
-   Studies not published in English.
Date-related:
-   Studies published before January 1, 2012.
Full-Text Screening Specific Exclusions:
-   Studies that do not contain empirical health indicators for the general, housed population (when a comparison is implied or attempted).
-   Studies that do not provide a method for comparing health indicator(s) between the exposed (homeless) and control (general housed) groups (e.g., missing denominators for calculating rates or risks).
    ```

-   **Environment Variables:**
    - The script will need database connection URLs defined in `docs/environment-vars.md` to interact with the Supabase-hosted `postgres` (prototype/development) database. It should use the standard application configuration to load these (e.g., from `.env` or `src/sr_assistant/app/config.py`).

-   **Coding Standards Notes:**
    - Follow all standards in [`docs/coding-standards.md`](/docs/coding-standards.md).
    - Implement robust logging using Loguru as per `py/python-logging-rules.mdc`.
    - Ensure the script is executable from the command line (e.g., `uv run python tools/seed_benchmark_data.py`).
    - The script should handle potential file reading errors for `bechmark-protocol.md` (only if it's still needed for other minor details, otherwise this dependency is reduced if all content is embedded here).

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests. Follow general testing approach in [`docs/testing-strategy.md`](/docs/testing-strategy.md).

-   **Unit Tests:**
    - Test logic for constructing the `criteria_framework_answers` JSONB object using the pre-defined PICO strings.
    - Test logic for constructing the XML-formatted, newline-delimited string for `SystematicReview.inclusion_criteria` from the PICO elements.
    - Test logic for preparing the pre-defined `SystematicReview.exclusion_criteria` string.
    - Test the logic for constructing `SystematicReviewCreate` / `SystematicReviewUpdate` Pydantic models using this data.
    - Mock database interactions (`ReviewService` or `SystematicReviewRepository` calls) to verify the correct data is passed for creation/update.
    - Test idempotency logic (e.g., checking if a protocol already exists).
-   **Integration Tests:**
    - Test the full script execution against the **Supabase-hosted `sra_integration_test` database** (ensure it's used and properly managed by `tests/conftest.py` if run as part of the test suite).
    - Verify that a `SystematicReview` record is correctly created or updated in the `sra_integration_test` database.
    - Verify that PICO elements are stored as **plain text** in `criteria_framework_answers` under `snake_case` keys, matching the strings provided in this story.
    - Verify that `SystematicReview.inclusion_criteria` stores the **XML-formatted, newline-delimited string** derived from these PICO elements, using `snake_case` XML tags.
    - Verify that `SystematicReview.exclusion_criteria` stores the newline-delimited plain text list as provided in this story.
-   **Manual/CLI Verification:**
    - Run the script locally: `uv run python tools/seed_benchmark_data.py`.
    - Inspect the `systematic_reviews` table in the **Supabase-hosted `postgres` (prototype/development) database** to confirm the benchmark protocol record is correctly seeded/updated per the specifications in this story.
    - Check script output for success/failure messages and logged actions.

## Tasks / Subtasks

-   [x] Task 1: Modify `tools/seed_benchmark_data.py` for the `seed_benchmark_protocol()` function.
    - [x] Subtask 1.1: Define the PICO text segments (Population, Intervention, Comparison, Outcome) as constants/literals within the script, using the exact text provided in this story's "Data Structures" section.
    - [x] Subtask 1.2: Implement logic to populate the `criteria_framework_answers` field of `SystematicReviewCreate`/`Update` with these PICO texts under `snake_case` keys (e.g., `population`, `intervention`).
    - [x] Subtask 1.3: Implement logic to construct the XML-formatted, newline-delimited string for `SystematicReview.inclusion_criteria` from these PICO texts, using `snake_case` tags (e.g., `<population>`, `<intervention>`).
    - [x] Subtask 1.4: Define the comprehensive exclusion criteria list (as provided in this story's "Data Structures" section) as a constant/literal newline-delimited string within the script and use it to populate `SystematicReview.exclusion_criteria`.
    - [x] Subtask 1.5: Remove any previous logic that parsed PICO or comprehensive exclusion criteria from `docs/benchmark/bechmark-protocol.md` (the script may still read it for other general inclusion points if needed, but not for PICO or the main exclusion block which are now hardcoded from this story).
-   [x] Task 2: Implement logic to interact with `ReviewService` or `SystematicReviewRepository` to check for existing benchmark protocol (for idempotency).
-   [x] Task 3: Implement logic to create a new `SystematicReview` record using `SystematicReviewCreate` or update an existing one using `SystematicReviewUpdate`, storing data as specified.
-   [x] Task 4: Add comprehensive Loguru logging and clear console output.
-   [x] Task 5: Create a main execution block in `tools/seed_benchmark_data.py` to run `seed_benchmark_protocol()`.
-   [x] Task 6: Write unit tests for the new logic, focusing on correct data assembly and DB interaction mocks.
-   [x] Task 7: (Optional) Write/update integration test.
-   [x] Task 8: Manually verify script functionality.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - Revision 1: Clarified PICO storage in `criteria_framework_answers` (plain text, no XML) and `inclusion_criteria`/`exclusion_criteria` string fields (newline-delimited plain text), based on user feedback. Noted deviation from PRD FR1.1 regarding XML in `criteria_framework_answers`.
    - Revision 2: Updated to reflect that `SystematicReview.inclusion_criteria` DB field _will_ store an XML-formatted, newline-delimited string derived from PICO components in `criteria_framework_answers`, for direct use by existing prompts. `criteria_framework_answers` itself stores plain PICO text. `exclusion_criteria` remains plain text. Adjusted ACs, Data Structures, and Tasks accordingly.
    - Revision 3: Updated keys in `criteria_framework_answers` to be descriptive (e.g., `POPULATION`) and corresponding XML tags in `inclusion_criteria` to match (e.g., `<POPULATION>`).
    - Revision 4: Changed PICO keys in `criteria_framework_answers` and corresponding XML tags in `inclusion_criteria` to `snake_case` (e.g., `population`, `<population>`).
    - Revision 5: Embedded the exact PICO text strings directly into the story for `criteria_framework_answers`. Clarified that `SystematicReview.inclusion_criteria` is built from these and that exclusion criteria are still parsed from the doc. Adjusted tasks.
    - Revision 6: Embedded the full, pre-defined PICO and exclusion criteria text directly into the story, removing the script's need to parse these specific sections from `docs/benchmark/bechmark-protocol.md`. Adjusted User Story, Detailed Requirements, ACs, Data Structures, and Tasks.

## Completion Notes / Dev Diary

-   **2025-05-20 (Developer Agent):**
    -   Refactored `tools/seed_benchmark_data.py` as per Task 1 requirements:
        - `parse_protocol_and_create_review()`: Updated to use pre-defined PICO/Exclusion text from story. Constructs `inclusion_criteria` (XML) and `exclusion_criteria` (newline-delimited text). Populates `criteria_framework_answers`.
        - `infer_source_db()`: Implemented logic based on 'key' field format (rayyan-_, pmid__, numeric).
        - `parse_csv_and_create_search_results()`: Updated to parse all relevant CSV fields, handle missing optional data, map 'key' to `source_id`, infer `source_db`, and store human benchmark decision in `source_metadata`.
        - `seed_data_to_db()`: Modified to delete existing `SearchResult` records for the benchmark review ID before deleting the review itself, then adds new data.
        - Main block (`if __name__ == "__main__":`) updated to orchestrate these functions. Logger configured for console (DEBUG) and file (INFO).
    -   Created unit tests in `tests/unit/tools/test_seed_benchmark_data.py` (Task 6):
        - Covered `parse_protocol_and_create_review` (PICO/exclusion data structure, XML format).
        - Covered `infer_source_db` logic for various key types.
        - Covered `parse_csv_and_create_search_results` (CSV parsing, data mapping, error handling for bad rows - mocked).
        - Covered `seed_data_to_db` (mocked DB interactions for add/delete/commit/refresh).
        - Covered `main` execution flow (mocked helper functions).
    -   **Unit Test Debugging:**
        -   Resolved `ModuleNotFoundError: No module named 'tools'` by:
            - Initially trying `sys.path` manipulation (ineffective).
            - Adding `__init__.py` to `tests/unit/tools/` (partially helpful).
            - Configuring `tool.pytest.ini_options.pythonpath` in `pyproject.toml` to include ".", "src/sr_assistant", and "tools", which fully resolved the import issue.
        -   Fixed minor assertion errors in unit tests related to logging calls and mock object interactions.
    -   Created integration tests in `tests/integration/tools/test_seed_benchmark_data_integration.py` (Task 2).
    -   **Integration Test Debugging (Extensive):**
        -   Encountered `sqlalchemy.exc.OperationalError: (psycopg.OperationalError) connection is bad / SSL connection has been closed unexpectedly`.
            - Investigated `_cleanup_db` and script's session handling.
            - Experimented with batched deletions and bulk deletes in `_cleanup_db`.
        -   Faced `psycopg.errors.FeatureNotSupported: cached plan must not change result type` when using SQLAlchemy Core `delete`. Reverted to ORM delete.
        -   Identified a critical issue: `psycopg.errors.InvalidTextRepresentation: invalid input value for enum searchdatabasesource_enum: "benchmark_csv"`.
            - The Python `SearchDatabaseSource` enum included "benchmark_csv", but the database enum did not.
            - Generated Alembic migration `1e530067ad44_add_other_to_searchdatabasesource_enum.py` to add "Other", "Embase", and "benchmark_csv" to the DB enum.
            - Ensured Alembic commands run with `ENVIRONMENT=prototype` (for live DB) or `ENVIRONMENT=test` (for test script subprocess).
        -   Discovered that the integration test DB (`sra_integration_test`) was being set up by `SQLModel.metadata.create_all()` in `tests/conftest.py`, not by Alembic migrations. This meant the enum changes from migrations weren't reflected in the test DB.
            - Modified `tests/conftest.py clean_db` fixture to run `alembic upgrade head` with `ENVIRONMENT=test`.
            - This led to a cascade of failures in older migrations (`3c121a82c373_searchresult_consolidation.py`, `d122fe7364e6_check_schema_sync_after_refactor.py`) due to missing tables/indexes/constraints they tried to drop.
            - Attempted to make these old migrations more robust by adding `if_exists=True` to `drop_index` and `try-except ProgrammingError` around `drop_constraint` and `drop_column`.
            - Due to persistent `InFailedSqlTransaction` errors even with `try-except`, indicating an unrecoverable transaction state after the first error in a migration script, reverted `tests/conftest.py clean_db` to use `SQLModel.metadata.create_all()` for simplicity and to bypass problematic historical migrations for test setup. This assumes current SQLModels define the correct schema for tests.
        -   Resolved `NameError: name 'sys' is not defined` in `tools/seed_benchmark_data.py` by adding `import sys`.
        -   Encountered `sqlalchemy.exc.InvalidRequestError: Could not refresh instance '<SystematicReview ...>'` in the script when run by tests. This was due to the `SystematicReview` object being created and refreshed in a session within `parse_protocol_and_create_review`, then passed to `seed_data_to_db` which used a different session.
        -   Refactored `tools/seed_benchmark_data.py` so `parse_protocol_and_create_review` now receives the main session from the `if __name__ == "__main__":` block, ensuring all DB operations for review and search results occur in the same session. This change is pending re-testing.
        -   The PICO and Exclusion criteria text in `parse_protocol_and_create_review` was inadvertently reverted during an edit; this has been restored to the detailed version from Story 4.1. Linter errors related to `SystematicReview` instantiation with `**kwargs` are present but suspected to be Pyright being overly strict; awaiting test run.
    -   **Current Status & Next Steps:**
        -   Integration tests are running but failing due to actual test logic issues (not setup):
            - `test_seeding_script_creates_and_populates_data_correctly`: Fails with `sqlalchemy.exc.InvalidRequestError: Could not refresh instance` in the script, indicating the session passing refactor needs to be verified.
            - `test_seeding_script_is_idempotent_on_rerun`: Also likely affected by the above, and also showed `AssertionError: assert 585 == 68` for search result counts previously.
            - `test_seeding_script_handles_missing_csv_gracefully`: Failing with an assertion on `review_before_script_run.exclusion_criteria`, possibly due to cleanup issues or test logic.
        -   The immediate next step is to re-run integration tests to confirm if the session-passing refactor in `tools/seed_benchmark_data.py` fixes the `InvalidRequestError`.
        -   Then, address the remaining test failures concerning data correctness and idempotency for `SearchResult` records, and the graceful CSV handling test. This will likely involve further refining the create/update logic in `parse_csv_and_create_search_results` and `seed_data_to_db`.

    -   **2025-05-20 (Developer Agent - Continued):**
        -   **Session Management Refactor:**
            - The `sqlalchemy.exc.InvalidRequestError: Could not refresh instance '<SystematicReview ...>'` was confirmed to be due to `parse_protocol_and_create_review` creating its own session. This was refactored so that the main execution block creates a single session and passes it to both `parse_protocol_and_create_review` and `seed_data_to_db`.
            - Subsequent `InvalidRequestError: Instance '<SystematicReview ...>' has been deleted` appeared. This was resolved by ensuring that `seed_data_to_db` correctly handles the (potentially already committed and refreshed) `SystematicReview` object from `parse_protocol_and_create_review`, and by committing deletions of old `SearchResult` records immediately.
        -   **Bulk Deletes & Idempotency Fixes:**
            - Implemented bulk delete for `SearchResult` records in `seed_data_to_db` using `sqlalchemy.delete()` and `db.execute()` to improve efficiency and potentially resolve some intermittent connection/transaction errors observed earlier. This also helped with idempotency.
            - Refined `_cleanup_db` in integration tests to use bulk deletes for both `SearchResult` and `SystematicReview` for consistency and robustness.
        -   **CSV Data Discrepancy Discovery & Test Adaptation:**
            - A major breakthrough in debugging test failures (`AssertionError: SearchResult with source_id pmid_33882220 not found`) came from inspecting the actual content of `docs/benchmark/human-reviewer-results-to-bench-against.csv`.
            - It was discovered that the CSV file currently in the workspace has keys like `rayyan-XXXXXXXXX` (e.g., `rayyan-388371190`) and a different column structure than previously assumed or used in earlier stages of development for this story. The expected `pmid_33882220` key was not present.
            - The integration tests (`test_seeding_script_creates_and_populates_data_correctly` and `test_seeding_script_is_idempotent_on_rerun`) were updated to assert against data from the _actual_ first record (`rayyan-388371190`) of this current CSV. This involved changing the expected `source_id`, `title`, `authors`, `year`, `source_db`, and `source_metadata` values in the assertions.
        -   **Final Test Success:**
            - After adapting the integration tests to the correct data in the current CSV, all unit tests and all integration tests for `tools/seed_benchmark_data.py` are now passing.
        -   **Linter Error Resolution:**
            -   Addressed persistent Pyright linter errors in both `tools/seed_benchmark_data.py` and `tests/integration/tools/test_seed_benchmark_data_integration.py`:
                - Changed `SystematicReview` instantiation in the script to use `model_validate({"id": ..., **data})` for better type handling.
                - Used general `# pyright: ignore` for `where` clauses where specific ignores were not effective for SQLAlchemy/SQLModel expressions that Pyright misinterprets.
                - Switched `db.exec(delete_stmt)` to `db.execute(delete_stmt)` in the script for `Delete` statements, adding appropriate ignores for `reportDeprecated` (SQLModel's preference for `exec`) and `reportUnknownMemberType`/`reportAttributeAccessIssue` for `result_proxy.rowcount`.
    -   **All primary objectives for `tools/seed_benchmark_data.py` refactoring and testing (unit & integration) for Story 4.1 are now complete.**
