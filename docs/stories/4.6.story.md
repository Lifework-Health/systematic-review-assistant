# Story 4.6: Benchmark UI - Load and Display Protocol

**Status:** Done

## Goal & Context

**User Story:** As a Benchmark User, I want a UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`) that loads and displays the details of the seeded benchmark protocol so that I can understand the context (research question, criteria) of the benchmark before running it.

**Context:** This story creates the initial user interface for the benchmarking module. It depends on the benchmark protocol being seeded into the database (US4.1). It allows the user to view the protocol against which the benchmark will be run. This supports PRD FR4.1 and FR4.2 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

- Create a new Streamlit page, for example, `human_benchmark_page.py` within the `src/sr_assistant/benchmark/pages/` directory.
- This page should be accessible from the main Streamlit application navigation (e.g., added to a list of pages in `main.py` or via Streamlit's native multi-page app capabilities if configured).
- On loading, the page must fetch the specific benchmark `SystematicReview` record from the database. This record can be identified by its pre-defined `BENCHMARK_REVIEW_ID` (from `tools/seed_benchmark_data.py`).
- The UI must display the following information from the fetched `SystematicReview` record:
    - Research Question (`research_question` field)
    - Background (`background` field, if available)
    - **PICO Elements**: Display the content of `criteria_framework_answers` (which contains plain text for population, intervention, comparison, outcome under keys like `population`, `intervention`, etc.). The XML tags that were used to construct `inclusion_criteria` should **not** be displayed here; show the plain text content for readability.
    - Full Inclusion Criteria (`inclusion_criteria` field): Display the XML-formatted string as stored in the database for user reference, perhaps in an expander or a less prominent way, as it's mainly for LLM consumption.
    - Full Exclusion Criteria (`exclusion_criteria` field, newline-delimited plain text).
- If the benchmark protocol record cannot be found, display an appropriate error message (e.g., "Benchmark protocol not found. Please ensure data has been seeded.").

## Acceptance Criteria (ACs)

- AC1: A new Streamlit page for the benchmark module is created (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`) and is navigable.
- AC2: The page successfully fetches the benchmark `SystematicReview` record from the database using its known ID.
- AC3: The UI correctly displays the Research Question and Background from the benchmark protocol.
- AC4: The UI displays the PICO elements (Population, Intervention, Comparison, Outcome) from `criteria_framework_answers` in a readable, plain text format (XML tags stripped for display if they were present in the values, though the story for 4.1 now specifies plain text values in this JSONB).
- AC5: The UI displays the full `inclusion_criteria` (the XML-formatted string) and `exclusion_criteria` (the newline-delimited plain text string) from the benchmark protocol.
- AC6: An informative error message is shown if the benchmark protocol record is not found.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**
    - File to Create: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or similar name)
    - File to Modify (potentially): `src/sr_assistant/app/main.py` or equivalent for adding the new page to navigation.
    - Files to Read From (for reference): `src/sr_assistant/core/models.py` (for `SystematicReview`), `src/sr_assistant/core/schemas.py` (for `SystematicReviewRead`).
    - Interaction with: `src/sr_assistant/app/services.py` (specifically `ReviewService` to fetch the review).
    - `tools/seed_benchmark_data.py` (to know the `BENCHMARK_REVIEW_ID`).

- **Key Technologies:**
    - Python 3.12
    - Streamlit
    - SQLModel (via `ReviewService`)

- **API Interactions / SDK Usage:**
    - Use `ReviewService.get_review(review_id: BENCHMARK_REVIEW_ID)` to fetch the benchmark protocol.

- **UI/UX Notes:**
    - Present the information clearly. Use `st.header`, `st.subheader`, `st.markdown`, `st.expander` as appropriate.
    - For PICO elements from `criteria_framework_answers`, display each component (Population, Intervention, etc.) with its text under a clear heading.
    - The raw XML `inclusion_criteria` might be best in an `st.expander` titled "Raw Inclusion Criteria (for LLM)".

- **Data Structures:**
    - `SystematicReviewRead` Pydantic schema will be the primary data structure received from the `ReviewService`.
    - The `criteria_framework_answers` field (a dict) will be iterated to display PICO elements. The keys are `population`, `intervention`, `comparison`, `outcome`.

- **Environment Variables:**
    - Standard application environment variables for database connection (used by `ReviewService`).

- **Coding Standards Notes:**
    - Adhere to Streamlit best practices for page structure.
    - Use Loguru for any necessary logging.

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

- **Unit Tests:**
    - Not heavily applicable for a display-only Streamlit page if logic is minimal. If helper functions are created for formatting data for display, those should be unit-tested.
- **Integration Tests (using `AppTest`):**
    - Test 1: Mock `ReviewService.get_review` to return a populated `SystematicReviewRead` object. Verify that all protocol details (RQ, background, PICO from `criteria_framework_answers` as plain text, XML `inclusion_criteria`, plain text `exclusion_criteria`) are correctly displayed in the UI elements (`st.markdown`, `st.text`, `st.expander`, etc.).
    - Test 2: Mock `ReviewService.get_review` to return `None`. Verify that the appropriate error message is displayed.
- **Manual/CLI Verification:**
    - Run the Streamlit app (`uv run streamlit run src/sr_assistant/app/main.py`).
    - Navigate to the new benchmark page.
    - Verify that the seeded benchmark protocol (from US4.1) is displayed correctly and matches the data in the database.
    - Verify readability of PICO elements and the presentation of inclusion/exclusion criteria.

## Tasks / Subtasks

- [x] Task 1: Create the new Streamlit page file (e.g., `src/sr_assistant/app/pages/human_benchmark_page.py`).
- [x] Task 2: Add the new page to the application's navigation structure.
- [x] Task 3: Implement logic to fetch the benchmark `SystematicReview` by its fixed ID using `ReviewService.get_review()`.
    - [x] Subtask 3.1: Handle the case where the review is not found and display an error message.
- [x] Task 4: Implement UI elements to display the research question and background.
- [x] Task 5: Implement UI elements to display PICO elements from `criteria_framework_answers` in a readable, plain text format.
- [x] Task 6: Implement UI elements to display the full `inclusion_criteria` (XML string) and `exclusion_criteria` (newline-delimited plain text).
- [x] Task 7: Write `AppTest` integration tests to verify correct data display and error handling.
- [x] Task 8: Manually test the page by running the Streamlit application.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** Claude 4 Sonnet
- **Completion Notes:**
    - Successfully created the human benchmark page (`src/sr_assistant/app/pages/human_benchmark_page.py`) that displays the benchmark protocol details including research question, background, PICO elements, and inclusion/exclusion criteria
    - Added the page to the main navigation under a new "Benchmark" section alongside the existing benchmark tool page
    - The existing `benchmark_tool.py` was left intact as it serves a different purpose (running benchmarks and analyzing results) versus this new page which displays protocol details for context
    - Implemented comprehensive error handling for cases where the benchmark protocol is not found
    - Created full integration test suite using `AppTest` covering successful display, error handling, and edge cases
    - All acceptance criteria (AC1-AC6) have been met
    - The page properly handles the JSONB `criteria_framework_answers` field and displays PICO elements in a user-friendly format
    - **SIGNIFICANT LAYOUT IMPROVEMENTS**: Based on user feedback about poor layout, completely redesigned the page with:
        - Modern visual design with emojis and better visual hierarchy
        - Organized content into logical sections: Protocol Overview, PICO Framework, Study Selection Criteria, Protocol Summary Dashboard
        - PICO elements displayed in an attractive 2x2 grid layout with clear labels
        - Categorized exclusion criteria for better readability (Language, Study Design, Publication Type, Other)
        - Interactive Protocol Summary Dashboard with metrics and readiness scoring
        - Protocol readiness indicator with completion checklist
        - Better use of Streamlit components: columns, metrics, expanders, containers
        - Enhanced error messaging and user guidance
        - Color-coded status indicators and warning/success messages
    - Added Protocol Summary section with metrics showing review details and status
    - Integration tests updated to match the improved layout structure and all passing
    - Manual testing confirmed significantly improved user experience and layout
- **Change Log:**
    - Initial implementation completed
    - All linter issues resolved  
    - Integration tests passing
    - Navigation structure updated with Benchmark section
    - Layout significantly improved based on user feedback: Complete redesign with modern UI, better organization, visual hierarchy, and enhanced user experience
