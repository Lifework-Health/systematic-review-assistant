# Story 4.12: Benchmark Results Export - Summary Metrics

**Status:** Done

## Goal & Context

**User Story:** As a Benchmark User, I want a download button on the UI for a selected `BenchmarkRun` so that I can download a CSV file of the summary performance metrics for that run.

**Context:** This story provides functionality to export the summary performance metrics of a benchmark run. It complements US4.11 (detailed export) and relies on the metrics being calculated and stored in `BenchmarkRun` (US4.8) and the UI page (US4.6/US4.9). This supports PRD FR6.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

- On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), provide a button labeled "Download Summary Metrics CSV".
- This button should be active or relevant only when a `BenchmarkRun` has been completed and its summary metrics are calculated and stored.
- When clicked, the system must:
    1. Fetch the selected/latest completed `BenchmarkRun` record (including all its metric columns).
    2. Construct a CSV file containing a single data row (plus a header row).
    3. The columns in the CSV MUST correspond to all the metric fields stored in the `BenchmarkRun` table (e.g., `id`, `created_at`, `benchmark_review_id`, `config_details`, `run_notes`, `tp, fp, fn, tn, sensitivity, specificity, accuracy, ppv, npv, f1_score, mcc_score, cohen_kappa, pabak, lr_plus, lr_minus`). The order should be logical.
    4. The CSV file should be offered to the user for download. The filename could be `benchmark_summary_metrics_run_[RUN_ID_OR_TIMESTAMP].csv`.

## Acceptance Criteria (ACs)

- AC1: A "Download Summary Metrics CSV" button is present on the benchmark UI page.
- AC2: Clicking the button triggers the generation and download of a CSV file.
- AC3: The CSV file contains one header row and one data row.
- AC4: The columns in the CSV match the metric and relevant metadata fields from the `BenchmarkRun` table (ID, timestamp, review ID, config, notes, and all calculated metrics like TP, FP, sensitivity, etc.).
- AC5: Data in the CSV accurately reflects the data from the selected `BenchmarkRun` record in the database.
- AC6: The download button is appropriately enabled/disabled based on the availability of a completed benchmark run with calculated metrics.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py`.
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunRead`).
    - Database interaction likely via repository methods (e.g., `BenchmarkRunRepository.get_by_id()` or `get_latest_completed()`).

- **Key Technologies:**
    - Python 3.12
    - Streamlit (for `st.download_button`)
    - Pandas (useful for creating a single-row DataFrame for easy CSV conversion).
    - SQLModel.

- **API Interactions / SDK Usage:**
    - Database interaction to fetch the `BenchmarkRun` record.

- **UI/UX Notes:**
    - Place the download button logically, perhaps near the summary metrics display (US4.9) or near the detailed export button (US4.11).

- **Data Structures:**
    - A `BenchmarkRunRead` Pydantic schema object.
    - `config_details` (JSONB) might need to be stringified or flattened if included directly, or a key aspect of it chosen for the summary CSV.

- **Environment Variables:**
    - Standard application environment variables for database connection.

- **Coding Standards Notes:**
    - Ensure all metric fields from `BenchmarkRun` are included in the CSV.
    - Handle `None` values in metric fields appropriately for CSV output (e.g., empty string or "N/A").

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

- **Unit Tests:**
    - Test any helper function that fetches the `BenchmarkRun` data.
    - Test any function that formats the `BenchmarkRun` data into the structure required for the CSV (e.g., handling JSON `config_details`).
    - Mock database calls.
- **Integration Tests (using `AppTest`):**
    - Setup: Seed a completed `BenchmarkRun` record (with all metric fields populated) in the `sra_integration_test` DB.
    - Test 1: Simulate clicking the "Download Summary Metrics CSV" button.
        - As with US4.11, `AppTest` cannot verify file downloads directly. The test should verify data preparation.
        - Intercept or redirect the data intended for `st.download_button`.
        - Assert that the generated CSV content (as a string or single-row DataFrame) has the correct headers and data from the seeded `BenchmarkRun`.
- **Manual/CLI Verification:**
    - After a benchmark run and metrics calculation have completed (US4.7, US4.8):
        - Navigate to the benchmark UI page.
        - Click the "Download Summary Metrics CSV" button.
        - Open the downloaded CSV and verify its columns and content against the `benchmark_runs` table in the database.

## Tasks / Subtasks

- [ ] Task 1: Implement logic in `human_benchmark_page.py` to fetch the selected/latest completed `BenchmarkRun` record.
- [ ] Task 2: Implement a function to prepare the `BenchmarkRun` data for CSV export.
    - [ ] Subtask 2.1: Ensure all required metric fields and relevant metadata are included.
    - [ ] Subtask 2.2: Handle formatting of complex fields like `config_details` (e.g., serialize as JSON string or pick key elements).
    - [ ] Subtask 2.3: Convert data to a format suitable for `st.download_button` (e.g., Pandas DataFrame then `to_csv(index=False)`).
- [ ] Task 3: Add an `st.download_button` to the UI for summary metrics.
    - [ ] Subtask 3.1: Configure label, filename, MIME type.
    - [ ] Subtask 3.2: Pass prepared CSV data string to the button.
    - [ ] Subtask 3.3: Implement logic for enabling/disabling based on run status.
- [ ] Task 4: Write unit tests for data fetching and CSV data preparation.
- [ ] Task 5: Write `AppTest` integration tests to verify data prepared for download.
- [ ] Task 6: Manually test CSV download and verify contents.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
- **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
