# Story 4.2: Seed Benchmark Dataset

**Status:** Done

## Goal & Context

**User Story:** As a Developer, I want the script (`tools/seed_benchmark_data.py`) to parse the benchmark dataset CSV (`docs/benchmark/human-reviewer-results-to-bench-against.csv`) so that I can create/update corresponding `SearchResult` records linked to the benchmark `SystematicReview`.

**Context:** This story builds upon US4.1, where the benchmark protocol (`SystematicReview` record) was seeded. Now, the actual dataset items (abstracts with human decisions) need to be loaded into the `search_results` table. This is essential for conducting benchmark runs. This directly supports PRD FR1.2 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

- The script (`tools/seed_benchmark_data.py`) must be extended to include functionality to parse `docs/benchmark/human-reviewer-results-to-bench-against.csv`.
- For each row in the CSV, a `SearchResult` record should be created and linked to the benchmark `SystematicReview` (identified, for example, by a specific known title or a unique ID stored during protocol seeding).
- The `source_db` field for these `SearchResult` records MUST be inferred. If the CSV does not contain explicit information to determine the original database (e.g., PubMed, Scopus), it should default to `SearchDatabaseSource.OTHER` (from `src/sr_assistant/core/types.py`).
- The human ground truth decision (e.g., `true`/`false`/`null` or `included`/`excluded`/`uncertain` from the CSV) MUST be stored in the `SearchResult.source_metadata` JSONB field, specifically under a key like `"benchmark_human_decision"`. The value stored should be a boolean (`True` for include, `False` for exclude) if possible, or a string if the source data is not easily convertible to boolean.
- The script must handle potential missing values in the CSV gracefully for non-critical fields of `SearchResult` (e.g., if 'journal' or 'year' is missing for an entry, it should be stored as `None`). Critical fields like 'title' or a unique identifier from the CSV (if available and used for idempotency) must be present.
- The script must be idempotent for dataset seeding. This means if the script is run multiple times, it should not create duplicate `SearchResult` records for the same benchmark review. This could be achieved by:
    - Deleting all existing `SearchResult` records linked to the specific benchmark `SystematicReview` ID before seeding new ones.
    - Or, if a unique key for each abstract exists in the CSV (e.g., 'key' column), checking for the existence of a `SearchResult` with that `source_id` (and `source_db = OTHER` if that's the default) for the benchmark review ID before creating a new one.
- The script should clearly indicate success or failure of the dataset seeding process and log its actions.

## Acceptance Criteria (ACs)

- AC1: Script correctly parses `docs/benchmark/human-reviewer-results-to-bench-against.csv` and creates `SearchResult` records linked to the previously seeded benchmark `SystematicReview` ID.
- AC2: `SearchResult.source_db` is correctly inferred or defaulted to `SearchDatabaseSource.OTHER`.
- AC3: `SearchResult.source_metadata` correctly stores `{"benchmark_human_decision": <parsed_human_decision>}` for each record, where `<parsed_human_decision>` is determined from the `exclusion_stage_round1` column as follows:
    - If `exclusion_stage_round1` contains "Title/Abstract": store `false` (excluded at title/abstract)
    - Otherwise: store `true` (passed title/abstract screening, regardless of full-text outcome)
    - Note: "Full text screen" exclusions are still stored as `true` since they passed title/abstract screening
    - Expected results: ~365 records with `false`, ~220 records with `true`
- AC4: The script handles missing optional data in the CSV by storing `None` for corresponding `SearchResult` fields without failing.
- AC5: Script is idempotent for dataset seeding (verified by running the script multiple times and checking for duplicate `SearchResult` records for the benchmark review).
- AC6: The script outputs clear success or failure messages for the dataset seeding part and logs its main actions (e.g., "X benchmark search results seeded for review Y", "Error parsing CSV row Z").

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**
    - File to Modify: `tools/seed_benchmark_data.py`
    - Files to Read From: `docs/benchmark/human-reviewer-results-to-bench-against.csv`
    - Files to Interact With (Models/Schemas): `src/sr_assistant/core/models.py` (specifically `SearchResult`, `SystematicReview`), `src/sr_assistant/core/schemas.py` (for `SearchResultCreate`).
    - Potentially `src/sr_assistant/app/services.py` (if using `SearchService` or `ReviewService`).

- **Key Technologies:**
    - Python 3.12
    - Pandas (for reading and processing the CSV file is recommended)
    - SQLModel/SQLAlchemy (for DB interaction)
    - Pydantic (for `SearchResultCreate` schema)
    - Loguru (for logging)

- **API Interactions / SDK Usage:**
    - Database interactions to create `SearchResult` records. This will likely involve using `SearchService.search_pubmed_and_store_results` (if adaptable, though it's geared for API fetches) or more likely `SearchResultRepository.add_all` (or a similar bulk insert method if creating many records).
    - Database interaction to fetch the benchmark `SystematicReview` ID.

- **UI/UX Notes:**
    - Not applicable for this script-based story.

- **Data Structures:**
    - `SearchResult` SQLModel from `src/sr_assistant/core/models.py`.
    - `SearchResultCreate` Pydantic schema from `src/sr_assistant/core/schemas.py`.
    - The CSV file `docs/benchmark/human-reviewer-results-to-bench-against.csv` needs to be parsed. Key columns to map to `SearchResult` model fields include (but are not limited to):
        - `key` (as `source_id` if `source_db` is `OTHER`)
        - `title`
        - `year`
        - `journal`
        - `abstract`
        - `authors` (needs parsing if it's a single string)
        - `keywords` (needs parsing if it's a single string)
        - `doi`
        - `pubmed_id` (if available, can inform `source_id` and `source_db` override to `PUBMED`)
        - `included_round1` or `included_round2` (or similar column indicating human decision for `benchmark_human_decision`). The script needs to clearly define which column(s) from the CSV map to the human ground truth and how to interpret their values (e.g., 'Y'/'N', '1'/'0', 'Included'/'Excluded').

- **Environment Variables:**
    - Database connection URLs from `docs/environment-vars.md`.

- **Coding Standards Notes:**
    - Follow all standards in [`docs/coding-standards.md`](/docs/coding-standards.md).
    - Implement robust logging using Loguru as per `py/python-logging-rules.mdc`.
    - The script should be executable from the command line (e.g., `uv run python tools/seed_benchmark_data.py --seed-dataset`). Consider adding command-line arguments to trigger different seeding actions (protocol vs. dataset).
    - Handle potential file reading/parsing errors for the CSV gracefully.
    - **Idempotency Strategy:** Decide on a clear strategy. Deleting existing benchmark search results for the review ID before re-seeding is often simplest. If using an update-or-insert strategy, a unique key from the CSV (like the `key` column if it's a persistent unique ID for each abstract) mapped to `source_id` is essential.

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests. Follow general testing approach in [`docs/testing-strategy.md`](/docs/testing-strategy.md).

- **Unit Tests:**
    - Test CSV parsing logic with a sample CSV string or small file.
    - Test mapping logic from CSV rows to `SearchResultCreate` Pydantic models.
    - Test logic for inferring `source_db`.
    - Test logic for extracting and formatting `benchmark_human_decision` into `source_metadata`.
    - Mock database interactions (`SearchService` or `SearchResultRepository` calls) to verify correct data is passed for creation.
    - Test idempotency logic (e.g., if clearing existing records, verify the delete call; if upserting, verify existing records are not duplicated).
- **Integration Tests:**
    - Test the full dataset seeding part of the script execution against the **Supabase-hosted `sra_integration_test` database**.
    - Verify that `SearchResult` records are correctly created and linked to the benchmark `SystematicReview` in the `sra_integration_test` database.
    - Verify correct `source_db` and `source_metadata.benchmark_human_decision` values.
    - Verify idempotency by running the script multiple times.
- **Manual/CLI Verification:**
    - Run the script locally: `uv run python tools/seed_benchmark_data.py --seed-dataset` (or similar, depending on CLI arg design).
    - Inspect the `search_results` table in the **Supabase-hosted `postgres` (prototype/development) database** to confirm records are correctly seeded and linked.
    - Check script output for success/failure messages and logged actions.

## Tasks / Subtasks

- [x] Task 1: Extend `tools/seed_benchmark_data.py` to include a function like `seed_benchmark_dataset()`.
    - [x] Subtask 1.1: Add argument parsing to the script to allow selective seeding (e.g., `--seed-protocol`, `--seed-dataset`, `--seed-all`).
- [x] Task 2: Implement CSV parsing logic within `seed_benchmark_dataset()`.
    - [x] Subtask 2.1: Use Pandas to read `docs/benchmark/human-reviewer-results-to-bench-against.csv`.
    - [x] Subtask 2.2: Handle potential errors during CSV reading.
- [x] Task 3: Implement logic to fetch the ID of the benchmark `SystematicReview` (seeded in US4.1).
- [x] Task 4: Implement mapping from CSV rows to `SearchResultCreate` objects.
    - [x] Subtask 4.1: Map CSV columns to `SearchResult` fields (title, abstract, year, etc.).
    - [x] Subtask 4.2: Implement logic to infer `source_db` (default to `OTHER`, or `PUBMED` if `pubmed_id` is present and valid).
    - [x] Subtask 4.3: Implement logic to extract the human ground truth decision and store it in `source_metadata.benchmark_human_decision` (e.g., map 'Y'/'N' from CSV to `True`/`False`).
    - [x] Subtask 4.4: Handle missing optional values from CSV gracefully.
- [x] Task 5: Implement idempotency logic for dataset seeding.
    - [x] Subtask 5.1: Choose strategy (e.g., delete existing by review_id, or upsert based on a unique key from CSV like the `key` column if suitable for `source_id`).
    - [x] Subtask 5.2: Implement chosen strategy using `SearchResultRepository` or `SearchService`.
- [x] Task 6: Implement database persistence for new `SearchResult` records (e.g., using `SearchResultRepository.add_all`).
- [x] Task 7: Add comprehensive Loguru logging and console output for the dataset seeding process.
- [x] Task 8: Write unit tests for CSV parsing, data mapping, and idempotency logic.
- [x] Task 9: (Optional, if feasible) Write integration test for the dataset seeding script function (targeting `sra_integration_test` DB).
- [x] Task 10: Manually verify script functionality against the Supabase-hosted `postgres` (prototype/development) database.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `Dev Agent`
- **Completion Notes:** All functionality for this story was implemented and tested as part of Story 4.1. The `tools/seed_benchmark_data.py` script now handles both protocol and dataset seeding. Refer to the "Completion Notes / Dev Diary" in `docs/stories/4.1.story.md` for full details of implementation, debugging, and testing.
- **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - Status updated to Done, tasks marked complete, and completion notes added referencing Story 4.1.
