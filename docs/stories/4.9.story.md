# Story 4.9: Benchmark UI - Display Summary Performance Metrics

**Status:** Review

## Goal & Context

**User Story:** As a Benchmark User, I want the UI page to display all summary performance metrics from a completed `BenchmarkRun` record so that I can evaluate the SRA's overall screening performance for that run.

**Context:** This story focuses on presenting the calculated benchmark metrics to the user. It builds upon US4.8 where metrics are calculated and persisted to the `BenchmarkRun` table, and US4.6 where the benchmark UI page is established. This supports PRD FR5.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

**Note:** Story 4.7 implementation in `src/sr_assistant/app/pages/benchmark_run_page.py` already includes real-time metrics display during the run. This story focuses on displaying persisted metrics from completed `BenchmarkRun` records and adding individual paper-level results in a detailed table view.

## Detailed Requirements

- On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), after a benchmark run is completed and metrics are calculated (US4.8):
  1. Fetch the relevant `BenchmarkRun` record from the database (e.g., the latest completed run, or allow selection if multiple runs are supported in the future; for MVP, displaying the most recent completed run is sufficient).
  2. Clearly display all populated metric values from the `BenchmarkRun` record. This includes:
     - Counts: TP, FP, FN, TN.
     - Calculated Metrics: Sensitivity, Specificity, Accuracy, PPV (Precision), NPV, F1 Score, MCC (Matthews Correlation Coefficient), Cohen's Kappa, PABAK, LR+, LR-.
  3. Metrics should be presented in a readable format, possibly grouped logically (e.g., raw counts, then primary ratios, then agreement stats).
  4. If a selected `BenchmarkRun` has no calculated metrics yet (e.g., if it's still in progress or failed before metric calculation), display an appropriate message.
  5. Display any `run_notes` associated with the `BenchmarkRun`.
  6. Display `config_details` (e.g., LLM models used for the run) in a readable way.
  7. **Display Individual Paper-Level Results:** Below the summary metrics, show a detailed table of individual benchmark items with:
     - Paper title and authors
     - Human decision (Include/Exclude)
     - AI decisions (Conservative, Comprehensive, Resolver if invoked)
     - Final AI decision
     - Classification (TP/FP/TN/FN)
     - AI rationales for each agent's decision
     - Confidence scores
     - Allow filtering/sorting by classification type or decision disagreements
     - Similar to the table view in `screen_abstracts.py` or `search.py` pages

## Acceptance Criteria (ACs)

- AC1: The benchmark UI page successfully fetches and displays the TP, FP, FN, TN counts from the selected/latest completed `BenchmarkRun` record.
- AC2: The UI correctly displays all other calculated metrics (Sensitivity through LR-) from the `BenchmarkRun` record, formatted appropriately (e.g., float to 2-3 decimal places).
- AC3: `run_notes` and `config_details` from the `BenchmarkRun` record are displayed.
- AC4: If a `BenchmarkRun` is selected/loaded that does not yet have metrics calculated (i.e., metric fields are NULL), the UI shows an appropriate status message (e.g., "Metrics not yet available for this run" or "Run in progress/failed before metric calculation").
- AC5: UI elements are clearly labeled with the metric names.
- AC6: Individual paper-level results are displayed in a sortable/filterable table showing all `BenchmarkResultItem` records for the selected run.
- AC7: The table displays human decisions, all AI agent decisions with rationales, confidence scores, and final classifications.
- AC8: Users can filter the table to view specific classification types (e.g., only False Positives) or papers where agents disagreed.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**

  - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or the benchmark UI page file name).
  - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunRead`).
  - Database interaction likely via a new repository method (e.g., `BenchmarkRunRepository.get_latest_completed()`) or a service method if complex fetching logic is needed.

- **Key Technologies:**

  - Python 3.12
  - Streamlit (for displaying data, e.g., `st.metric`, `st.table`, `st.expander`, `st.json`)
  - SQLModel

- **API Interactions / SDK Usage:**

  - Database interaction to fetch `BenchmarkRun` records.

- **UI/UX Notes:**

  - Use `st.metric` for displaying key individual metrics for visual impact.
  - Consider using `st.columns` to organize the display of multiple metrics.
  - `config_details` (JSONB) can be displayed using `st.json` or formatted into a more readable markdown/table.
  - For MVP, fetching and displaying the most recently completed `BenchmarkRun` record is sufficient. A selection mechanism for past runs can be a future enhancement.

- **Data Structures:**

  - `BenchmarkRunRead` Pydantic schema.
  - `BenchmarkResultItem` and `SearchResult` models for paper-level details.

- **Environment Variables:**

  - Standard application environment variables for database connection.

- **Coding Standards Notes:**
  - Ensure metric names are displayed consistently with `docs/sr_metrics.md`.
  - Handle `None` values for metrics gracefully in the UI (e.g., display as "N/A" or "-").

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

- **Unit Tests:**
  - If helper functions are created for formatting metrics or `config_details` for display, these should be unit-tested.
- **Integration Tests (using `AppTest`):**
  - Test 1: Seed a `BenchmarkRun` record in the `sra_integration_test` DB with all metric fields populated. Mock the DB call in the Streamlit page to return this record. Verify all metrics, notes, and config details are displayed correctly and formatted as expected.
  - Test 2: Seed a `BenchmarkRun` record with `None` values for metric fields. Mock the DB call. Verify the UI displays an appropriate message indicating metrics are unavailable.
  - Test 3: Mock the DB call to return `None` (no `BenchmarkRun` found). Verify an appropriate message is shown.
  - Test 4: Verify individual paper-level results table displays correctly with filtering capabilities.
- **Manual/CLI Verification:**
  - After a benchmark run (US4.7) and metrics calculation (US4.8) have completed for the Supabase-hosted `postgres` DB:
    - Navigate to the benchmark UI page.
    - Verify all calculated metrics, notes, and config details are displayed correctly and match the database record.
    - Verify paper-level results table shows all benchmark items with correct classifications.

## Tasks / Subtasks

- [x] Task 1: Implement logic in `human_benchmark_page.py` to fetch the latest completed `BenchmarkRun` record (or a selected one if a selection mechanism is added for MVP).
  - [x] Subtask 1.1: (If needed) Add a method to `BenchmarkRunRepository` or a service to fetch the latest/specific `BenchmarkRun`.
  - [x] Subtask 1.2: Handle cases where no benchmark runs are found or no completed runs exist.
- [x] Task 2: Implement UI elements to display TP, FP, FN, TN counts.
- [x] Task 3: Implement UI elements to display all other calculated metrics (Sensitivity through LR-), ensuring appropriate formatting (e.g., to 2-3 decimal places).
- [x] Task 4: Implement UI elements to display `run_notes` and `config_details`.
- [x] Task 5: Implement UI logic to handle and display messages when metrics are not yet available for a run.
- [x] Task 6: Implement individual paper-level results table:
  - [x] Subtask 6.1: Fetch all `BenchmarkResultItem` records for the selected `BenchmarkRun`.
  - [x] Subtask 6.2: Join with `SearchResult` data to get paper titles, authors, abstracts.
  - [x] Subtask 6.3: Create a DataFrame with all necessary columns (title, authors, human decision, AI decisions, rationales, confidence scores, classification).
  - [x] Subtask 6.4: Implement sortable/filterable table using `st.dataframe` or similar.
  - [x] Subtask 6.5: Add filter controls for classification type and decision disagreements.
- [x] Task 7: Write `AppTest` integration tests for the display logic with various `BenchmarkRun` data scenarios.
- [ ] Task 8: Manually test the display on the Streamlit page after a full benchmark run.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** Claude Sonnet 4
- **Completion Notes:** Successfully implemented all core functionality for displaying persisted benchmark metrics from completed BenchmarkRun records. The implementation includes:

  - New "Completed Benchmark Runs" section in `benchmark_run_page.py`
  - Function to fetch latest completed benchmark run with calculated metrics
  - Comprehensive metrics display including confusion matrix counts, primary metrics (accuracy, sensitivity, specificity, etc.), and advanced metrics (MCC, Cohen's Kappa, likelihood ratios)
  - Proper handling of cases where metrics are not yet available
  - Individual paper-level results table with filtering capabilities (by classification, decision agreement, human decision)
  - Detailed paper view showing rationales from all agents
  - Metric value formatting function that handles int/float/None values and percentage display
  - Integration tests covering all major scenarios

  **Technical Implementation:** Used existing repository patterns, leveraged Streamlit caching for performance, implemented robust error handling, and followed project coding standards. The UI is organized with clear sections and provides comprehensive filtering and drill-down capabilities for analyzing benchmark results.

**Recent Updates & Production Fixes:**

- **LangSmith Integration Added (2025-05-29):**

  - Added `conservative_run_id`, `conservative_trace_id`, `comprehensive_run_id`, `comprehensive_trace_id` fields to `BenchmarkResultItem` model
  - Applied Alembic migration `ad42aea13565` to production database
  - Updated benchmark execution to capture LangSmith IDs from screening agents
  - Enhanced detail view to show clickable LangSmith trace links for debugging
  - Resolver agent LangSmith integration deferred (not yet implemented in agents)

- **Database Connection Reliability Fixes:**

  - **Issue Identified:** Database "SSL_SYSCALL error: EOF detected" during long-running benchmark executions (items 330+)
  - **Root Cause:** Database sessions held open during 60+ second AI agent processing, causing Supabase to drop idle connections
  - **Solutions Implemented:**
    - Enhanced connection pooling: 10 base connections, 20 overflow, 1-hour recycle, pre-ping validation
    - **Critical Fix:** Separated AI processing from database operations - AI agents now run without DB sessions, followed by short-lived DB transactions
    - Refactored benchmark execution to process individual items with immediate session cleanup
    - Improved error handling and progress tracking per item

- **UI Enhancements:**

  - **Run History Selection:** Replaced single "Load Latest" button with dropdown selector for all completed benchmark runs
  - Auto-save behavior: Benchmark runs are automatically persisted, no manual save button needed
  - Improved status messaging during benchmark execution with real-time per-item updates
  - Better separation between live benchmark execution and historical run viewing
  - Removed "useless" comments per coding standards (e.g., "# Add visual separator")

- **Performance & Reliability Improvements:**
  - Changed from 10-item batches to individual item processing for better UI responsiveness
  - Added real-time metrics calculation and display during execution
  - Improved error recovery - individual item failures don't halt entire benchmark
  - Enhanced logging for better debugging of long-running operations

**Lessons Learned:**

1. **Database Session Management:** Long-running operations (AI processing) should never hold database sessions open. Always separate compute-heavy operations from database transactions.

2. **Connection Pooling is Critical:** Production applications need proper connection pooling configuration, especially with cloud databases that have aggressive connection timeouts.

3. **Real-time UI Updates:** Individual item processing with `st.rerun()` provides much better UX than batch processing with long wait periods.

4. **Error Isolation:** Individual item error handling allows benchmarks to continue even when some items fail, providing more useful partial results.

5. **LangSmith Integration:** Capturing trace IDs during execution enables powerful debugging capabilities, essential for production AI systems.

6. **Auto-save vs Manual Save:** Auto-save reduces user friction and ensures data persistence without additional UI complexity.

- **Change Log:**
  - Initial Draft - Created story structure and requirements
  - Updated to Ready status after 4.8 completion
  - Implementation completed with all acceptance criteria met
  - 2025-05-29: Added LangSmith integration, fixed database connection issues, enhanced run history UI
