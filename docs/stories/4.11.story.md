# Story 4.11: Benchmark Results Export - Detailed Items

**Status:** Draft

## Goal & Context

**User Story:** As a Benchmark User, I want a download button on the UI for a selected `BenchmarkRun` so that I can download a CSV file of the detailed `BenchmarkResultItem` data for that run.

**Context:** This story provides the functionality to export detailed, item-by-item results from a benchmark run, allowing for external analysis and reporting. It builds on US4.7 (creation of `BenchmarkResultItem` records) and US4.6 (the benchmark UI page). This supports PRD FR6.1 and FR6.2 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), provide a button labeled "Download Detailed Results CSV".
-   This button should be active or relevant only when a `BenchmarkRun` has been completed and its `BenchmarkResultItem` records are available.
-   When clicked, the system must:
    1. Fetch all `BenchmarkResultItem` records associated with the selected/latest completed `BenchmarkRun`.
    2. For each `BenchmarkResultItem`, also fetch the corresponding `SearchResult` record to get the `source_id` (original key, e.g., rayyan-xxxxx or pmid_xxxxx) and `title`.
    3. Construct a CSV file with the following columns, in this order:
        - `search_result_source_id` (from `SearchResult.source_id`)
        - `title` (from `SearchResult.title`)
        - `human_decision` (from `BenchmarkResultItem.human_decision`, ideally as "INCLUDE"/"EXCLUDE"/"UNCERTAIN" or True/False)
        - `conservative_decision` (from `BenchmarkResultItem.conservative_decision` enum value, e.g., "INCLUDE")
        - `comprehensive_decision` (from `BenchmarkResultItem.comprehensive_decision` enum value)
        - `resolver_decision` (from `BenchmarkResultItem.resolver_decision` enum value, blank if not resolved)
        - `final_decision` (SRA's output for the run, from `BenchmarkResultItem.final_decision` enum value)
        - `classification` (from `BenchmarkResultItem.classification`, e.g., "TP", "FP")
    4. The CSV file should be offered to the user for download. The filename could be something like `benchmark_detailed_results_run_[RUN_ID_OR_TIMESTAMP].csv`.

## Acceptance Criteria (ACs)

- AC1: A "Download Detailed Results CSV" button is present on the benchmark UI page.
- AC2: Clicking the button triggers the generation and download of a CSV file.
- AC3: The CSV file contains the specified columns: `search_result_source_id`, `title`, `human_decision`, `conservative_decision`, `comprehensive_decision`, `resolver_decision`, `final_decision`, `classification`.
- AC4: Each row in the CSV corresponds to a `BenchmarkResultItem` from the selected `BenchmarkRun`.
- AC5: Data in the CSV accurately reflects the data from the `BenchmarkResultItem` and linked `SearchResult` records in the database (e.g., decisions are correctly represented as strings like "INCLUDE").
- AC6: The download button is appropriately enabled/disabled based on the availability of a completed benchmark run.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py`.
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkResultItem`, `SearchResult`), `src/sr_assistant/core/schemas.py` (`BenchmarkResultItemRead`, `SearchResultRead`).
    - Database interaction likely via repository methods (e.g., `BenchmarkResultItemRepository.get_by_run_id_with_search_result_details()`) or a service.

-   **Key Technologies:**
    - Python 3.12
    - Streamlit (for `st.download_button`)
    - Pandas (highly recommended for creating the DataFrame to be converted to CSV).
    - SQLModel.

-   **API Interactions / SDK Usage:**
    - Database interaction to fetch `BenchmarkResultItem` records and their associated `SearchResult` details for a specific `BenchmarkRun`.

-   **UI/UX Notes:**
    - Place the download button logically within the display area for a completed benchmark run.

-   **Data Structures:**
    - A list of `BenchmarkResultItemRead` (potentially augmented with `SearchResult` details) or a Pandas DataFrame to hold the data before CSV conversion.
    - `ScreeningDecisionType` enum values should be converted to their string representations (e.g., `ScreeningDecisionType.INCLUDE.value`) for the CSV.
    - `human_decision` (boolean) should also be converted to a meaningful string (e.g., "INCLUDE"/"EXCLUDE" or "True"/"False").

-   **Environment Variables:**
    - Standard application environment variables for database connection.

-   **Coding Standards Notes:**
    - Ensure CSV generation is efficient, especially if benchmark datasets can be large.
    - Handle `None` values in decision fields appropriately (e.g., output as empty string or "N/A" in CSV).

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - Test any helper function that fetches and joins `BenchmarkResultItem` data with `SearchResult` data.
    - Test any function that formats the joined data into the structure required for the CSV (e.g., converting enums/booleans to strings).
    - Mock database calls.
-   **Integration Tests (using `AppTest`):**
    -   Setup: Seed a `BenchmarkRun`, associated `BenchmarkResultItem`s, and linked `SearchResult`s in the `sra_integration_test` DB.
    -   Test 1: Simulate clicking the "Download Detailed Results CSV" button.
        - `AppTest` cannot directly verify file downloads. The test should focus on verifying that the correct data is prepared for the `st.download_button`.
        - This can be done by: Modifying the Streamlit page code *during the test* (e.g., using `mocker.patch.object` or by having the download function return the CSV string/DataFrame instead of passing it to `st.download_button` when a test flag is set in `st.session_state`).
        - Assert that the generated CSV content (as a string or DataFrame) has the correct headers and data for the seeded items.
-   **Manual/CLI Verification:**
    -   After a benchmark run (US4.7) has completed:
        - Navigate to the benchmark UI page.
        - Click the "Download Detailed Results CSV" button.
        - Open the downloaded CSV file and verify its columns and content against the database records.

## Tasks / Subtasks

-   [ ] Task 1: Implement logic in `human_benchmark_page.py` to fetch `BenchmarkResultItem` records for the selected run, including necessary fields from linked `SearchResult` (like `source_id`, `title`).
    - [ ] Subtask 1.1: (If needed) Add a repository method to fetch this joined data efficiently.
-   [ ] Task 2: Implement a function to prepare the data for CSV export.
    - [ ] Subtask 2.1: Convert enum/boolean decisions to appropriate string representations.
    - [ ] Subtask 2.2: Structure data into a list of dictionaries or a Pandas DataFrame with the specified column order.
-   [ ] Task 3: Add an `st.download_button` to the UI.
    - [ ] Subtask 3.1: Configure the button with an appropriate label, filename, and MIME type (`text/csv`).
    - [ ] Subtask 3.2: Pass the prepared CSV data (as a string) to the `data` argument of the download button.
    - [ ] Subtask 3.3: Implement logic to enable/disable the button based on run completion.
-   [ ] Task 4: Write unit tests for data fetching and CSV data preparation logic.
-   [ ] Task 5: Write `AppTest` integration tests to verify the data prepared for download (by intercepting or redirecting the data from `st.download_button`).
-   [ ] Task 6: Manually test the CSV download and verify its contents.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes *within this specific story file* if iterations occur}
    - Initial Draft
    - ...
