# Story 4.14: Improve Benchmark Screening Prompts for Conservative Decision-Making

**Status:** Draft

## Goal & Context

**User Story:** As a Developer, I want to fine-tune the screening agent prompts to be more conservative in exclusion decisions, so that the AI screening system minimizes false negatives (incorrectly excluding relevant papers) in the benchmark evaluations.

**Context:** This story addresses critical feedback from human reviewers who observed the AI being too aggressive in exclusions during benchmark testing. The current prompts are resulting in excessive false negatives, where papers that should be included for full-text review are being excluded at the title/abstract screening stage. This story focuses on adjusting the screening agent prompts to adopt a more conservative stance, emphasizing that uncertainty should lead to inclusion rather than exclusion.

## Detailed Requirements

Based on human reviewer feedback from benchmark testing:

1. **"Absence of Evidence is Not Evidence of Absence"**: The AI should not exclude papers simply because certain information (e.g., country of study) is not explicitly mentioned in the abstract. When information is unclear or missing, err on the side of inclusion.

2. **Focus on Criteria Application**: The agents should directly apply the inclusion/exclusion criteria rather than trying to integrate or interpret through PICO framework concepts. While PICO provides helpful context, the actual screening decisions must be based on the specified inclusion/exclusion criteria.

3. **Cost of Errors**: The prompts must explicitly acknowledge that false negatives (excluding relevant papers) have a higher cost than false positives (including irrelevant papers) in systematic reviews.

## Acceptance Criteria (ACs)

- AC1: System prompts are updated to emphasize that "the absence of evidence is not evidence of absence" (e.g., papers where the country of study is unclear should be included for full-text review rather than excluded).
- AC2: Conservative and comprehensive agent prompts focus on applying the inclusion/exclusion criteria directly rather than trying to integrate PICO framework concepts.
- AC3: Prompts explicitly state that the cost of false negatives (excluding relevant papers) is higher than false positives (including irrelevant papers).
- AC4: Resolver agent prompt is updated to be consistent with the more conservative approach.
- AC5: Benchmark re-run shows improved sensitivity (reduced false negatives) compared to baseline results.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**

    - Files to Modify:
        - `src/sr_assistant/app/agents/screening_agents.py` (update prompts for conservative_screener, comprehensive_screener, and resolver)
        - `src/sr_assistant/app/agents/prompts.py` (if prompts are defined separately)

- **Key Technologies:**

    - LangChain for prompt templates
    - Anthropic Claude for LLM calls

- **Prompt Engineering Guidelines:**

    - Be explicit about conservative decision-making
    - Include examples of when to include despite missing information
    - Clarify that PICO is context but criteria are the decision basis
    - Use clear, unambiguous language

- **Data Structures:**

    - No new data structures required
    - Existing screening decision structures remain unchanged

- **Environment Variables:**

    - No new environment variables required

- **Coding Standards Notes:**

    - Follow existing prompt formatting conventions
    - Ensure prompts are well-documented with comments explaining the rationale
    - Keep prompt changes versioned/tracked for comparison

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests. Follow general testing approach in [`docs/testing-strategy.md`](/docs/testing-strategy.md).

- **Benchmark Testing:**
    - Run full benchmark using `tools/seed_benchmark_data.py` and benchmark UI
    - Compare results to baseline (current) performance
    - Focus on sensitivity metric (should increase) and false negative count (should decrease)
    - Document specific examples where the new prompts correctly include papers that were previously excluded

- **Manual Verification:**
    - Test with specific edge cases from the benchmark where country information is ambiguous
    - Verify that resolver maintains consistency with the conservative approach
    - Check that prompts are clear and actionable for the LLM

- **Metrics to Track:**
    - Sensitivity (True Positive Rate) - should increase
    - False Negatives - should decrease
    - Overall accuracy - should improve or remain stable

## Tasks / Subtasks

- [ ] Task 1: Analyze current prompts and identify areas for improvement
    - [ ] Review conservative_screener prompt
    - [ ] Review comprehensive_screener prompt
    - [ ] Review resolver prompt
    - [ ] Document current decision patterns leading to false negatives

- [ ] Task 2: Update conservative_screener prompt
    - [ ] Add explicit guidance about "absence of evidence"
    - [ ] Focus on direct criteria application
    - [ ] Include cost of errors explanation
    - [ ] Add examples of conservative inclusion decisions

- [ ] Task 3: Update comprehensive_screener prompt
    - [ ] Align with conservative approach while maintaining thoroughness
    - [ ] Ensure consistency in handling missing information
    - [ ] Focus on criteria over PICO interpretation

- [ ] Task 4: Update resolver prompt
    - [ ] Ensure resolver favors inclusion when agents disagree
    - [ ] Add guidance for handling uncertainty
    - [ ] Maintain consistency with conservative philosophy

- [ ] Task 5: Run benchmark and compare results
    - [ ] Execute full benchmark run with updated prompts
    - [ ] Compare metrics to baseline results
    - [ ] Document improvements in sensitivity and false negative reduction
    - [ ] Identify any unexpected changes in other metrics

- [ ] Task 6: Document prompt changes and rationale
    - [ ] Create comparison of old vs new prompts
    - [ ] Document specific improvements and their justification
    - [ ] Update any prompt documentation or comments

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:**
- **Completion Notes:**
- **Change Log:**
    - Initial Draft created based on human reviewer feedback
