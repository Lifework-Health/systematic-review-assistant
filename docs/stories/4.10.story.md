# Story 4.10: Benchmark UI - Display AI Confidence Stats

**Status:** Draft

## Goal & Context

**User Story:** As a Benchmark User, I want the UI page to display statistics for AI confidence scores so that I can understand the distribution and general confidence levels of the AI reviewers for a selected benchmark run.

**Context:** This story enhances the benchmark results UI by providing insights into the confidence scores of the AI screening agents. It builds upon US4.7 (where `BenchmarkResultItem` records containing confidence scores are created) and US4.6 (the benchmark UI page). This supports PRD FR5.4 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), after a benchmark run is completed and its results are being displayed (as per US4.9):
    1. Fetch all `BenchmarkResultItem` records associated with the selected/latest completed `BenchmarkRun`.
    2. For the fetched items, extract the confidence scores for:
        a.  Conservative reviewer (`conservative_confidence` field).
        b.  Comprehensive reviewer (`comprehensive_confidence` field).
        c.  Resolver agent (`resolver_confidence` field, if applicable and available for a sufficient number of items).
    3. Calculate and display summary statistics for each of these sets of confidence scores. Suggested statistics include:
        - Mean
        - Median
        - Minimum
        - Maximum
        - Standard Deviation
        - Interquartile Range (IQR) or specific quartiles (25th, 75th percentile).
    4. Alternatively, or additionally, display a simple histogram or distribution plot for each set of confidence scores to visualize their distribution.
    5. If no confidence scores are available for a particular reviewer/agent (e.g., resolver was not invoked often, or scores were not recorded), display an appropriate message (e.g., "Confidence scores not available for resolver in this run").
    6. The display of these statistics should be clearly associated with the selected `BenchmarkRun`.

## Acceptance Criteria (ACs)

- AC1: The benchmark UI page correctly fetches `BenchmarkResultItem` records for the selected run.
- AC2: Summary statistics (Mean, Median, Min, Max, Std Dev, IQR/Quartiles) for `conservative_confidence` are calculated and displayed.
- AC3: Summary statistics (Mean, Median, Min, Max, Std Dev, IQR/Quartiles) for `comprehensive_confidence` are calculated and displayed.
- AC4: Summary statistics (Mean, Median, Min, Max, Std Dev, IQR/Quartiles) for `resolver_confidence` are calculated and displayed if sufficient data exists; otherwise, an appropriate message is shown.
- AC5: Optionally, histograms or distribution plots for each set of confidence scores are displayed.
- AC6: UI elements for confidence statistics are clearly labeled.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or the benchmark UI page file name).
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkResultItem`), `src/sr_assistant/core/schemas.py` (`BenchmarkResultItemRead`).
    - Database interaction likely via a new repository method (e.g., `BenchmarkResultItemRepository.get_by_run_id()`) or a service method.

-   **Key Technologies:**
    - Python 3.12
    - Streamlit (for displaying data, e.g., `st.subheader`, `st.table`, `st.dataframe`, `st.altair_chart` or `st.pyplot` for histograms if implemented).
    - Pandas (highly recommended for calculating statistics from a list of confidence scores).
    - NumPy (often used with Pandas for statistical calculations).
    - SQLModel.

-   **API Interactions / SDK Usage:**
    - Database interaction to fetch `BenchmarkResultItem` records for a specific `BenchmarkRun`.

-   **UI/UX Notes:**
    - Present statistics in a clear table or using individual `st.metric` for key stats.
    - Histograms, if used, should be simple and easy to interpret.
    - Place this section logically within the benchmark results display, perhaps under an expander.

-   **Data Structures:**
    - List of `BenchmarkResultItemRead` Pydantic schemas or a Pandas DataFrame derived from them.

-   **Environment Variables:**
    - Standard application environment variables for database connection.

-   **Coding Standards Notes:**
    - Ensure statistical calculations are correct and handle cases with no data (e.g., no resolver confidence scores) gracefully.

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - Test any helper functions created for calculating specific statistics (if not using Pandas/NumPy directly for all).
    - Test logic that extracts confidence scores and prepares them for statistical functions or plotting.
-   **Integration Tests (using `AppTest`):**
    - Setup: Seed a `BenchmarkRun` and several `BenchmarkResultItem` records in `sra_integration_test` DB with varying confidence scores (including some `None` values, especially for resolver).
    - Test 1: Mock the DB call in the Streamlit page to return these seeded items. Verify that all calculated summary statistics for conservative, comprehensive, and resolver confidence scores are correctly displayed and match manually calculated values.
    - Test 2: If histograms are implemented, mock DB calls and verify that chart elements are rendered (specific content validation might be limited by `AppTest`).
    - Test 3: Mock DB calls to return items where, e.g., all `resolver_confidence` scores are `None`. Verify that the UI shows an appropriate message.
-   **Manual/CLI Verification:**
    -   After a benchmark run (US4.7) has populated `BenchmarkResultItem` records in the Supabase-hosted `postgres` DB:
        - Navigate to the benchmark UI page.
        - Verify that the displayed confidence statistics appear correct and plausible based on the underlying data.

## Tasks / Subtasks

-   [ ] Task 1: Implement logic in `human_benchmark_page.py` to fetch all `BenchmarkResultItem` records for the selected/latest `BenchmarkRun`.
    - [ ] Subtask 1.1: (If needed) Add a method to `BenchmarkResultItemRepository` or a service to fetch items by `benchmark_run_id`.
-   [ ] Task 2: Extract lists of `conservative_confidence`, `comprehensive_confidence`, and `resolver_confidence` scores from the fetched items (handle `None` values, e.g., by filtering them out before calculation or ensuring stats functions can handle them).
-   [ ] Task 3: Calculate summary statistics (Mean, Median, Min, Max, Std Dev, IQR/Quartiles) for each set of scores, likely using Pandas.
-   [ ] Task 4: Implement UI elements (e.g., `st.table`, `st.metric`) to display these statistics clearly labeled for each reviewer type.
-   [ ] Task 5: (Optional) Implement UI elements (e.g., `st.altair_chart` or `st.pyplot` with Matplotlib/Seaborn via Pandas plotting) to display histograms of the confidence scores.
-   [ ] Task 6: Handle cases where confidence data might be insufficient or absent for a category (e.g., resolver).
-   [ ] Task 7: Write `AppTest` integration tests for the confidence statistics display.
-   [ ] Task 8: Manually test the display on the Streamlit page.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
