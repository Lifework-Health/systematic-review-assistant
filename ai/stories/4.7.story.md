# Story 4.7: Benchmark UI - Trigger Benchmark Run & Process Items

**Status:** Draft

## Goal & Context

**User Story:** As a Benchmark User, I want a button "Run New Benchmark Screening" on the UI page so that I can initiate the AI screening pipeline (direct agent calls) for the entire benchmark dataset.

**Context:** This story implements the core benchmark execution logic. It builds on the UI page created in US4.6 (where the button will reside) and uses the seeded protocol (US4.1) and dataset (US4.2). It will create `BenchmarkRun` (US4.3) and `BenchmarkResultItem` (US4.4) records. This directly supports PRD FR4.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), add a button labeled "Run New Benchmark Screening".
-   Clicking this button MUST:
    1. Create a new `BenchmarkRun` record in the database. This record should be timestamped (`created_at`, `updated_at`). The `config_details` field (JSONB) should be populated with relevant configuration information for this run (e.g., names of LLMs used for conservative, comprehensive, and resolver agents, if these are easily accessible or configurable at this stage; otherwise, can be a placeholder or omit if too complex for MVP. PRD FR4.3a). `benchmark_review_id` must be linked to the seeded benchmark protocol.
    2. Fetch all `SearchResult` records associated with the benchmark `SystematicReview` ID.
    3. For each fetched benchmark `SearchResult`:
        a.  Directly call the `screen_abstracts_batch()` function (from `src/sr_assistant/app/agents/screening_agents.py`) with the `SearchResult` data and the benchmark `SystematicReview` protocol. This function will return conservative and comprehensive screening decisions/results.
        b.  Determine if there is a conflict or uncertainty based on the SRA's established resolver logic (e.g., one `INCLUDE` & one `EXCLUDE`; or one definitive decision & one `UNCERTAIN`; or both `UNCERTAIN` - as per expanded resolver scope in Epic 3 / `docs/prd-resolver.md` v1.1 if that applies, or stick to PRD FR4.3b if only clear conflicts are resolved for MVP).
        c.  If a conflict/uncertainty meeting criteria for resolution exists, directly call `invoke_resolver_chain()` (from `src/sr_assistant/app/agents/screening_agents.py`) with the necessary inputs (search result, review protocol, conservative & comprehensive results) to get a `resolver_decision`.
        d.  Determine the `final_decision_for_benchmark_item`: This will be the `resolver_decision` if the resolver was invoked and returned a definitive decision. If the resolver was not invoked (no conflict/uncertainty meeting criteria), it will be the agreed-upon decision from conservative/comprehensive. If the resolver was invoked but returned `UNCERTAIN`, this should be reflected as the `final_decision_for_benchmark_item`.
        e.  Calculate the `classification` (e.g., "TP", "FP", "TN", "FN") by comparing the `final_decision_for_benchmark_item` with the `human_decision` stored in `SearchResult.source_metadata.benchmark_human_decision`.
        f.  Create and store a new `BenchmarkResultItem` record in the database, linking it to the current `BenchmarkRun` ID and the `SearchResult` ID. This record must store: `human_decision`, `conservative_decision` details, `comprehensive_decision` details, `resolver_decision` details (if applicable), the `final_decision_for_benchmark_item` (as `final_decision`), and the `classification`.
    4. The UI should display meaningful progress updates during the run (e.g., "Processing item X of Y", "Screening abstracts...", "Resolving conflicts...", "Storing results...").
    5. After all items are processed, proceed to metrics calculation (covered in US4.8).

## Acceptance Criteria (ACs)

-   AC1: Clicking "Run New Benchmark Screening" button creates a new `BenchmarkRun` record in the database, linked to the correct benchmark `SystematicReview` ID, with `created_at` and `updated_at` timestamps, and `config_details` populated (even if minimally for MVP, e.g., noting fixed LLM models used).
-   AC2: For each `SearchResult` in the benchmark dataset, the `screen_abstracts_batch()` function is successfully called directly.
-   AC3: For `SearchResult` items with conflicting or uncertain (per defined logic) decisions from the initial screeners, `invoke_resolver_chain()` is successfully called directly.
-   AC4: For each `SearchResult`, a `BenchmarkResultItem` record is created and persisted, containing:
    - Correct `benchmark_run_id` and `search_result_id`.
    - The `human_decision` (from `SearchResult.source_metadata`).
    - The `conservative_decision`, `conservative_confidence`, `conservative_rationale`.
    - The `comprehensive_decision`, `comprehensive_confidence`, `comprehensive_rationale`.
    - The `resolver_decision`, `resolver_confidence`, `resolver_reasoning` (if resolver was invoked).
    - The correct `final_decision` (SRA's overall output for the item in this run).
    - The correct `classification` (TP, FP, TN, FN) based on `final_decision` vs `human_decision`.
-   AC5: The UI provides clear progress updates during the benchmark run execution.
-   AC6: All operations are performed within a single database session for the entire benchmark run to ensure transactional integrity for creating the `BenchmarkRun` and all its `BenchmarkResultItem` records.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or the benchmark UI page file name from US4.6).
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`, `BenchmarkResultItem`, `SearchResult`, `SystematicReview`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunCreate`, `BenchmarkResultItemCreate`), `src/sr_assistant/app/agents/screening_agents.py` (`screen_abstracts_batch`, `invoke_resolver_chain`).
    - Database interaction will likely be through repositories (`BenchmarkRunRepository`, `BenchmarkResultItemRepository` - to be created or use generic `BaseRepository` methods) or directly via a session if keeping logic within the Streamlit page script.

-   **Key Technologies:**
    - Python 3.12
    - Streamlit (for UI button and progress display)
    - SQLModel, SQLAlchemy (for DB operations)
    - LangChain (for direct agent calls)

-   **API Interactions / SDK Usage:**
    - Direct Python calls to `screen_abstracts_batch()` and `invoke_resolver_chain()`.
    - Database interactions to create `BenchmarkRun` and `BenchmarkResultItem` records.

-   **UI/UX Notes:**
    - Use `st.button` for triggering the run.
    - Use `st.progress()` or `st.status()` / `st.spinner()` to show progress.
    - Log detailed progress to the console/log file.

-   **Data Structures:**
    - `BenchmarkRunCreate` for the new run.
    - `BenchmarkResultItemCreate` for each item.
    - Logic to compare `ScreeningDecisionType` from conservative and comprehensive screeners to determine if resolver should be called.
    - Logic to map `ScreeningResult` (from `screen_abstracts_batch`) and `ResolverOutputSchema` (from `invoke_resolver_chain`) to `BenchmarkResultItemCreate` fields.
    - Human decision from `SearchResult.source_metadata.benchmark_human_decision`.

-   **Environment Variables:**
    - Standard application environment variables for database connection and LLM API keys (used by the agent functions).

-   **Coding Standards Notes:**
    - Follow all standards in [`docs/coding-standards.md`](/docs/coding-standards.md).
    - Adhere to logging rules from `py/python-logging-rules.mdc`.
    - Ensure database operations for a single benchmark run are handled transactionally.

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - Test the logic for identifying conflicts/uncertainty requiring resolver invocation.
    - Test the logic for determining `final_decision_for_benchmark_item` under various scenarios (agreement, conflict_resolved, conflict_uncertain_after_resolver).
    - Test the logic for calculating `classification` (TP, FP, TN, FN).
    - Mock `screen_abstracts_batch`, `invoke_resolver_chain`, and database repository/session calls to test the orchestration logic within the UI page function.
-   **Integration Tests (using `AppTest` for Streamlit page):**
    -   Setup: Seed benchmark protocol (US4.1) and a small, representative benchmark dataset (US4.2) in the `sra_integration_test` DB.
    -   Test 1: Simulate clicking the "Run New Benchmark Screening" button.
        - Mock `screen_abstracts_batch` and `invoke_resolver_chain` to return predefined outputs (including conflicts and agreements).
        - Verify that a `BenchmarkRun` record is created in the test DB.
        - Verify that the correct number of `BenchmarkResultItem` records are created.
        - Verify the content of a few sample `BenchmarkResultItem` records (AI decisions, final_decision, classification) based on the mocked LLM outputs.
    -   Test 2 (More complex, optional for MVP if direct LLM calls are too slow/costly for CI): Allow limited, actual LLM calls for a very small number of items (1-2) to test the full pipeline through LLM interaction, then mock the rest. This would require careful API key management for tests.
-   **Manual/CLI Verification:**
    - Run the Streamlit app.
    - Navigate to the benchmark page.
    - Click "Run New Benchmark Screening".
    - Observe UI progress indicators.
    - After completion, inspect the `benchmark_runs` and `benchmark_result_items` tables in the Supabase-hosted `postgres` DB to verify correct data persistence and all fields populated as expected (including all decision fields and classification).

## Tasks / Subtasks

-   [ ] Task 1: Add a "Run New Benchmark Screening" button to `human_benchmark_page.py`.
-   [ ] Task 2: Implement the main benchmark execution function triggered by the button click.
    -   [ ] Subtask 2.1: Implement logic to create a new `BenchmarkRun` record (using `BenchmarkRunCreate` and repository/session) and store its ID.
    -   [ ] Subtask 2.2: Fetch the benchmark `SystematicReview` and its associated `SearchResult` records.
    -   [ ] Subtask 2.3: Loop through each `SearchResult`:
        - [ ] Subtask 2.3.1: Call `screen_abstracts_batch()` (handle its output, which includes conservative and comprehensive results).
        - [ ] Subtask 2.3.2: Implement logic to check for disagreement/uncertainty that triggers the resolver (based on PRD FR4.3b or updated resolver scope).
        - [ ] Subtask 2.3.3: If resolver is needed, call `invoke_resolver_chain()`.
        - [ ] Subtask 2.3.4: Determine `final_decision_for_benchmark_item` and `classification`.
        - [ ] Subtask 2.3.5: Prepare `BenchmarkResultItemCreate` data with all AI decision details, human decision, final decision, and classification.
        - [ ] Subtask 2.3.6: Store the `BenchmarkResultItem` record (using repository/session).
    -   [ ] Subtask 2.4: Implement UI progress updates (`st.progress` or `st.status`).
    -   [ ] Subtask 2.5: Ensure all database operations for one run occur within a single session/transaction.
-   [ ] Task 3: Write unit tests for conflict identification, final decision logic, and classification logic.
-   [ ] Task 4: Write `AppTest` integration tests for the benchmark run trigger and basic result persistence (mocking LLM calls initially).
-   [ ] Task 5: Manually test the full benchmark run process.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
