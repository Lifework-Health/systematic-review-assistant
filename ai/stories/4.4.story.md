# Story 4.4: Define `BenchmarkResultItem` Model and Schemas

**Status:** Draft

## Goal & Context

**User Story:** As a Developer, I want to define the `BenchmarkResultItem` SQLModel and corresponding Pydantic schemas (`Base`, `Create`, `Read`) so that individual item screening outcomes from a benchmark run (human decision vs. all AI decisions, classification) can be persisted and accessed.

**Context:** This story defines the database structure and data transfer objects for storing the results of each individual abstract screened during a benchmark run. This data is critical for calculating performance metrics. It depends on US4.3 (BenchmarkRun model definition) as `BenchmarkResultItem` will link to a `BenchmarkRun`. This directly supports PRD FR3.2 and FR3.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   Define a new SQLModel class `BenchmarkResultItem` in `src/sr_assistant/core/models.py`.
    -   It must include `id` (UUID, primary key, default_factory=uuid.uuid4), `created_at` (AwareDatetime, default_factory=utcnow, nullable=False), and `updated_at` (AwareDatetime, default_factory=utcnow, sa_column_kwargs={"onupdate": utcnow}, nullable=False).
    -   It must have `benchmark_run_id` (UUID, ForeignKey to `benchmark_runs.id`).
    -   It must have `search_result_id` (UUID, ForeignKey to `search_results.id`).
    -   It must include `human_decision` (BOOLEAN, nullable=True) to store the ground truth.
    -   Include fields to store the SRA's decisions and related data for this item during the specific benchmark run:
        - `conservative_decision: ScreeningDecisionType | None = Field(default=None)`
        - `conservative_confidence: float | None = Field(default=None)`
        - `conservative_rationale: str | None = Field(default=None)`
        - `comprehensive_decision: ScreeningDecisionType | None = Field(default=None)`
        - `comprehensive_confidence: float | None = Field(default=None)`
        - `comprehensive_rationale: str | None = Field(default=None)`
        - `resolver_decision: ScreeningDecisionType | None = Field(default=None)` (if resolver was invoked)
        - `resolver_confidence: float | None = Field(default=None)`
        - `resolver_reasoning: str | None = Field(default=None)`
    -   It must include `final_decision: ScreeningDecisionType` (This is the SRA's overall output decision for this item for this benchmark run, determined after conservative, comprehensive, and potentially resolver steps. It should be non-nullable once a result is recorded).
    -   It must include `classification: str` (e.g., "TP", "FP", "TN", "FN", non-nullable once recorded).
-   Define Pydantic schemas in `src/sr_assistant/core/schemas.py` for `BenchmarkResultItem`:
    - `BenchmarkResultItemBase(BaseSchema)`: Contains all fields common to create and read.
    - `BenchmarkResultItemCreate(BenchmarkResultItemBase)`: For creating new items. `benchmark_run_id`, `search_result_id`, `final_decision`, and `classification` should be mandatory. Other AI decision fields (conservative, comprehensive, resolver) are optional as they depend on the AI's output process.
    - `BenchmarkResultItemRead(BenchmarkResultItemBase)`: For returning item data, including `id`, `created_at`, `updated_at`.
-   All SQLModel fields and Pydantic schema fields MUST have field docstrings as per `docs/coding-standards.md` and `docs/data-models.md#Pydantic-Field-Documentation-Standard`.
-   Ensure all Pydantic schemas inherit from `core.schemas.BaseSchema`.
-   Update `docs/data-models.md` to include the new `BenchmarkResultItem` SQLModel and Pydantic schemas, and update the ERD.

## Acceptance Criteria (ACs)

- AC1: `BenchmarkResultItem` SQLModel is defined in `src/sr_assistant/core/models.py` with all specified fields (`id`, timestamps, FKs, `human_decision`, conservative/comprehensive/resolver decision details, `final_decision`, `classification`).
- AC2: Pydantic schemas `BenchmarkResultItemBase`, `BenchmarkResultItemCreate`, `BenchmarkResultItemRead` are defined in `src/sr_assistant/core/schemas.py`, inheriting from `BaseSchema` and accurately reflecting the `BenchmarkResultItem` model structure with appropriate optionality.
- AC3: All fields in both the SQLModel and Pydantic schemas have clear field docstrings.
- AC4: `docs/data-models.md` is updated with the `BenchmarkResultItem` model and schema definitions and a revised ERD showing its relationship to `BenchmarkRun` and `SearchResult`.
- AC5: Code passes all linter checks (Ruff, Pyright).

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Create/Modify: `src/sr_assistant/core/models.py` (add `BenchmarkResultItem` SQLModel)
    - File to Create/Modify: `src/sr_assistant/core/schemas.py` (add `BenchmarkResultItemBase`, `BenchmarkResultItemCreate`, `BenchmarkResultItemRead` Pydantic schemas)
    - File to Modify: `docs/data-models.md` (add new model/schema definitions and update ERD)

-   **Key Technologies:**
    - Python 3.12
    - SQLModel
    - Pydantic
    - Mermaid (for ERD in `docs/data-models.md`)

-   **API Interactions / SDK Usage:**
    - Not applicable for this definition story.

-   **UI/UX Notes:**
    - Not applicable for this definition story.

-   **Data Structures:**
    -   **`BenchmarkResultItem` SQLModel:** Refer to Detailed Requirements. `ScreeningDecisionType` from `core.types` should be used for decision fields. Confidence scores are floats. Rationales are strings.
    -   **Pydantic Schemas:** Structure as `Base`, `Create`, `Read`.
        - `BenchmarkResultItemCreate`: `benchmark_run_id`, `search_result_id`, `final_decision` (SRA's output), and `classification` are mandatory. `human_decision` is nullable bool. Other AI decision components are optional.
        - `BenchmarkResultItemRead`: Includes `id`, `created_at`, `updated_at`.

-   **Environment Variables:**
    - Not directly applicable for this definition story.

-   **Coding Standards Notes:**
    - Follow all standards in [`docs/coding-standards.md`](/docs/coding-standards.md).
    - Use `AwareDatetime` for Pydantic, `datetime` with `timezone.utc` for SQLModel timestamp defaults.
    - Field docstrings are mandatory.

## Testing Requirements

**Guidance:** Verify implementation against the ACs. For this story, testing is primarily through code review, linter checks, and successful generation of related artifacts (like DB migration in a subsequent story).

-   **Unit Tests:**
    - Not directly applicable for model/schema definitions, but Pydantic schemas will be implicitly tested when used in other unit tests.
    - Simple unit tests for Pydantic schema instantiation and validation can be written.
-   **Integration Tests:**
    - Successful database migration (US4.5) will validate the SQLModel.
-   **Manual/CLI Verification:**
    - Code review of `models.py` and `schemas.py`.
    - Review of the updated `docs/data-models.md`.
    - Successful execution of Ruff and Pyright linters.

## Tasks / Subtasks

-   [ ] Task 1: Define the `BenchmarkResultItem` SQLModel in `src/sr_assistant/core/models.py`.
    - [ ] Subtask 1.1: Add `id`, `benchmark_run_id` (FK), `search_result_id` (FK), `human_decision`, `created_at`, `updated_at` fields.
    - [ ] Subtask 1.2: Add fields for `conservative_decision`, `conservative_confidence`, `conservative_rationale`.
    - [ ] Subtask 1.3: Add fields for `comprehensive_decision`, `comprehensive_confidence`, `comprehensive_rationale`.
    - [ ] Subtask 1.4: Add fields for `resolver_decision`, `resolver_confidence`, `resolver_reasoning`.
    - [ ] Subtask 1.5: Add `final_decision` (SRA's output) and `classification` fields.
    - [ ] Subtask 1.6: Ensure correct table name (`benchmark_result_items`) and add field docstrings.
-   [ ] Task 2: Define the Pydantic schemas (`BenchmarkResultItemBase`, `BenchmarkResultItemCreate`, `BenchmarkResultItemRead`) in `src/sr_assistant/core/schemas.py`.
    - [ ] Subtask 2.1: Implement `BenchmarkResultItemBase`.
    - [ ] Subtask 2.2: Implement `BenchmarkResultItemCreate` (mandatory fields: `benchmark_run_id`, `search_result_id`, `final_decision`, `classification`).
    - [ ] Subtask 2.3: Implement `BenchmarkResultItemRead`.
    - [ ] Subtask 2.4: Ensure all Pydantic schemas inherit from `core.schemas.BaseSchema` and have field docstrings.
-   [ ] Task 3: Update `docs/data-models.md`.
    - [ ] Subtask 3.1: Add the definition of the `BenchmarkResultItem` SQLModel.
    - [ ] Subtask 3.2: Add definitions for the `BenchmarkResultItem` Pydantic schemas.
    - [ ] Subtask 3.3: Update the Mermaid ERD to include `BenchmarkResultItem` and its relationships to `BenchmarkRun` and `SearchResult`.
-   [ ] Task 4: Run linters (Ruff, Pyright) and fix any issues in the new/modified code.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
