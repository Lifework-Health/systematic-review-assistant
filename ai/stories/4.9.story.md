# Story 4.9: Benchmark UI - Display Summary Performance Metrics

**Status:** Draft

## Goal & Context

**User Story:** As a Benchmark User, I want the UI page to display all summary performance metrics from a completed `BenchmarkRun` record so that I can evaluate the SRA's overall screening performance for that run.

**Context:** This story focuses on presenting the calculated benchmark metrics to the user. It builds upon US4.8 where metrics are calculated and persisted to the `BenchmarkRun` table, and US4.6 where the benchmark UI page is established. This supports PRD FR5.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), after a benchmark run is completed and metrics are calculated (US4.8):
    1. Fetch the relevant `BenchmarkRun` record from the database (e.g., the latest completed run, or allow selection if multiple runs are supported in the future; for MVP, displaying the most recent completed run is sufficient).
    2. Clearly display all populated metric values from the `BenchmarkRun` record. This includes:
        - Counts: TP, FP, FN, TN.
        - Calculated Metrics: Sensitivity, Specificity, Accuracy, PPV (Precision), NPV, F1 Score, MCC (Matthews Correlation Coefficient), Cohen's Kappa, PABAK, LR+, LR-.
    3. Metrics should be presented in a readable format, possibly grouped logically (e.g., raw counts, then primary ratios, then agreement stats).
    4. If a selected `BenchmarkRun` has no calculated metrics yet (e.g., if it's still in progress or failed before metric calculation), display an appropriate message.
    5. Display any `run_notes` associated with the `BenchmarkRun`.
    6. Display `config_details` (e.g., LLM models used for the run) in a readable way.

## Acceptance Criteria (ACs)

- AC1: The benchmark UI page successfully fetches and displays the TP, FP, FN, TN counts from the selected/latest completed `BenchmarkRun` record.
- AC2: The UI correctly displays all other calculated metrics (Sensitivity through LR-) from the `BenchmarkRun` record, formatted appropriately (e.g., float to 2-3 decimal places).
- AC3: `run_notes` and `config_details` from the `BenchmarkRun` record are displayed.
- AC4: If a `BenchmarkRun` is selected/loaded that does not yet have metrics calculated (i.e., metric fields are NULL), the UI shows an appropriate status message (e.g., "Metrics not yet available for this run" or "Run in progress/failed before metric calculation").
- AC5: UI elements are clearly labeled with the metric names.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or the benchmark UI page file name).
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunRead`).
    - Database interaction likely via a new repository method (e.g., `BenchmarkRunRepository.get_latest_completed()`) or a service method if complex fetching logic is needed.

-   **Key Technologies:**
    - Python 3.12
    - Streamlit (for displaying data, e.g., `st.metric`, `st.table`, `st.expander`, `st.json`)
    - SQLModel

-   **API Interactions / SDK Usage:**
    - Database interaction to fetch `BenchmarkRun` records.

-   **UI/UX Notes:**
    - Use `st.metric` for displaying key individual metrics for visual impact.
    - Consider using `st.columns` to organize the display of multiple metrics.
    - `config_details` (JSONB) can be displayed using `st.json` or formatted into a more readable markdown/table.
    - For MVP, fetching and displaying the most recently completed `BenchmarkRun` record is sufficient. A selection mechanism for past runs can be a future enhancement.

-   **Data Structures:**
    - `BenchmarkRunRead` Pydantic schema.

-   **Environment Variables:**
    - Standard application environment variables for database connection.

-   **Coding Standards Notes:**
    - Ensure metric names are displayed consistently with `docs/sr_metrics.md`.
    - Handle `None` values for metrics gracefully in the UI (e.g., display as "N/A" or "-").

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - If helper functions are created for formatting metrics or `config_details` for display, these should be unit-tested.
-   **Integration Tests (using `AppTest`):**
    - Test 1: Seed a `BenchmarkRun` record in the `sra_integration_test` DB with all metric fields populated. Mock the DB call in the Streamlit page to return this record. Verify all metrics, notes, and config details are displayed correctly and formatted as expected.
    - Test 2: Seed a `BenchmarkRun` record with `None` values for metric fields. Mock the DB call. Verify the UI displays an appropriate message indicating metrics are unavailable.
    - Test 3: Mock the DB call to return `None` (no `BenchmarkRun` found). Verify an appropriate message is shown.
-   **Manual/CLI Verification:**
    -   After a benchmark run (US4.7) and metrics calculation (US4.8) have completed for the Supabase-hosted `postgres` DB:
        - Navigate to the benchmark UI page.
        - Verify all calculated metrics, notes, and config details are displayed correctly and match the database record.

## Tasks / Subtasks

-   [ ] Task 1: Implement logic in `human_benchmark_page.py` to fetch the latest completed `BenchmarkRun` record (or a selected one if a selection mechanism is added for MVP).
    - [ ] Subtask 1.1: (If needed) Add a method to `BenchmarkRunRepository` or a service to fetch the latest/specific `BenchmarkRun`.
    - [ ] Subtask 1.2: Handle cases where no benchmark runs are found or no completed runs exist.
-   [ ] Task 2: Implement UI elements to display TP, FP, FN, TN counts.
-   [ ] Task 3: Implement UI elements to display all other calculated metrics (Sensitivity through LR-), ensuring appropriate formatting (e.g., to 2-3 decimal places).
-   [ ] Task 4: Implement UI elements to display `run_notes` and `config_details`.
-   [ ] Task 5: Implement UI logic to handle and display messages when metrics are not yet available for a run.
-   [ ] Task 6: Write `AppTest` integration tests for the display logic with various `BenchmarkRun` data scenarios.
-   [ ] Task 7: Manually test the display on the Streamlit page after a full benchmark run.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
