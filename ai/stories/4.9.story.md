# Story 4.9: Benchmark UI - Display Summary Performance Metrics

**Status:** Ready

## Goal & Context

**User Story:** As a Benchmark User, I want the UI page to display all summary performance metrics from a completed `BenchmarkRun` record so that I can evaluate the SRA's overall screening performance for that run.

**Context:** This story focuses on presenting the calculated benchmark metrics to the user. It builds upon US4.8 where metrics are calculated and persisted to the `BenchmarkRun` table, and US4.6 where the benchmark UI page is established. This supports PRD FR5.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

**Note:** Story 4.7 implementation in `src/sr_assistant/app/pages/benchmark_run_page.py` already includes real-time metrics display during the run. This story focuses on displaying persisted metrics from completed `BenchmarkRun` records and adding individual paper-level results in a detailed table view.

## Detailed Requirements

- On the benchmark UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`), after a benchmark run is completed and metrics are calculated (US4.8):
    1. Fetch the relevant `BenchmarkRun` record from the database (e.g., the latest completed run, or allow selection if multiple runs are supported in the future; for MVP, displaying the most recent completed run is sufficient).
    2. Clearly display all populated metric values from the `BenchmarkRun` record. This includes:
        - Counts: TP, FP, FN, TN.
        - Calculated Metrics: Sensitivity, Specificity, Accuracy, PPV (Precision), NPV, F1 Score, MCC (Matthews Correlation Coefficient), Cohen's Kappa, PABAK, LR+, LR-.
    3. Metrics should be presented in a readable format, possibly grouped logically (e.g., raw counts, then primary ratios, then agreement stats).
    4. If a selected `BenchmarkRun` has no calculated metrics yet (e.g., if it's still in progress or failed before metric calculation), display an appropriate message.
    5. Display any `run_notes` associated with the `BenchmarkRun`.
    6. Display `config_details` (e.g., LLM models used for the run) in a readable way.
    7. **Display Individual Paper-Level Results:** Below the summary metrics, show a detailed table of individual benchmark items with:
        - Paper title and authors
        - Human decision (Include/Exclude)
        - AI decisions (Conservative, Comprehensive, Resolver if invoked)
        - Final AI decision
        - Classification (TP/FP/TN/FN)
        - AI rationales for each agent's decision
        - Confidence scores
        - Allow filtering/sorting by classification type or decision disagreements
        - Similar to the table view in `screen_abstracts.py` or `search.py` pages

## Acceptance Criteria (ACs)

- AC1: The benchmark UI page successfully fetches and displays the TP, FP, FN, TN counts from the selected/latest completed `BenchmarkRun` record.
- AC2: The UI correctly displays all other calculated metrics (Sensitivity through LR-) from the `BenchmarkRun` record, formatted appropriately (e.g., float to 2-3 decimal places).
- AC3: `run_notes` and `config_details` from the `BenchmarkRun` record are displayed.
- AC4: If a `BenchmarkRun` is selected/loaded that does not yet have metrics calculated (i.e., metric fields are NULL), the UI shows an appropriate status message (e.g., "Metrics not yet available for this run" or "Run in progress/failed before metric calculation").
- AC5: UI elements are clearly labeled with the metric names.
- AC6: Individual paper-level results are displayed in a sortable/filterable table showing all `BenchmarkResultItem` records for the selected run.
- AC7: The table displays human decisions, all AI agent decisions with rationales, confidence scores, and final classifications.
- AC8: Users can filter the table to view specific classification types (e.g., only False Positives) or papers where agents disagreed.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

- **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or the benchmark UI page file name).
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunRead`).
    - Database interaction likely via a new repository method (e.g., `BenchmarkRunRepository.get_latest_completed()`) or a service method if complex fetching logic is needed.

- **Key Technologies:**
    - Python 3.12
    - Streamlit (for displaying data, e.g., `st.metric`, `st.table`, `st.expander`, `st.json`)
    - SQLModel

- **API Interactions / SDK Usage:**
    - Database interaction to fetch `BenchmarkRun` records.

- **UI/UX Notes:**
    - Use `st.metric` for displaying key individual metrics for visual impact.
    - Consider using `st.columns` to organize the display of multiple metrics.
    - `config_details` (JSONB) can be displayed using `st.json` or formatted into a more readable markdown/table.
    - For MVP, fetching and displaying the most recently completed `BenchmarkRun` record is sufficient. A selection mechanism for past runs can be a future enhancement.

- **Data Structures:**
    - `BenchmarkRunRead` Pydantic schema.
    - `BenchmarkResultItem` and `SearchResult` models for paper-level details.

- **Environment Variables:**
    - Standard application environment variables for database connection.

- **Coding Standards Notes:**
    - Ensure metric names are displayed consistently with `docs/sr_metrics.md`.
    - Handle `None` values for metrics gracefully in the UI (e.g., display as "N/A" or "-").

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

- **Unit Tests:**
    - If helper functions are created for formatting metrics or `config_details` for display, these should be unit-tested.
- **Integration Tests (using `AppTest`):**
    - Test 1: Seed a `BenchmarkRun` record in the `sra_integration_test` DB with all metric fields populated. Mock the DB call in the Streamlit page to return this record. Verify all metrics, notes, and config details are displayed correctly and formatted as expected.
    - Test 2: Seed a `BenchmarkRun` record with `None` values for metric fields. Mock the DB call. Verify the UI displays an appropriate message indicating metrics are unavailable.
    - Test 3: Mock the DB call to return `None` (no `BenchmarkRun` found). Verify an appropriate message is shown.
    - Test 4: Verify individual paper-level results table displays correctly with filtering capabilities.
- **Manual/CLI Verification:**
    - After a benchmark run (US4.7) and metrics calculation (US4.8) have completed for the Supabase-hosted `postgres` DB:
        - Navigate to the benchmark UI page.
        - Verify all calculated metrics, notes, and config details are displayed correctly and match the database record.
        - Verify paper-level results table shows all benchmark items with correct classifications.

## Tasks / Subtasks

- [ ] Task 1: Implement logic in `human_benchmark_page.py` to fetch the latest completed `BenchmarkRun` record (or a selected one if a selection mechanism is added for MVP).
    - [ ] Subtask 1.1: (If needed) Add a method to `BenchmarkRunRepository` or a service to fetch the latest/specific `BenchmarkRun`.
    - [ ] Subtask 1.2: Handle cases where no benchmark runs are found or no completed runs exist.
- [ ] Task 2: Implement UI elements to display TP, FP, FN, TN counts.
- [ ] Task 3: Implement UI elements to display all other calculated metrics (Sensitivity through LR-), ensuring appropriate formatting (e.g., to 2-3 decimal places).
- [ ] Task 4: Implement UI elements to display `run_notes` and `config_details`.
- [ ] Task 5: Implement UI logic to handle and display messages when metrics are not yet available for a run.
- [ ] Task 6: Implement individual paper-level results table:
    - [ ] Subtask 6.1: Fetch all `BenchmarkResultItem` records for the selected `BenchmarkRun`.
    - [ ] Subtask 6.2: Join with `SearchResult` data to get paper titles, authors, abstracts.
    - [ ] Subtask 6.3: Create a DataFrame with all necessary columns (title, authors, human decision, AI decisions, rationales, confidence scores, classification).
    - [ ] Subtask 6.4: Implement sortable/filterable table using `st.dataframe` or similar.
    - [ ] Subtask 6.5: Add filter controls for classification type and decision disagreements.
- [ ] Task 7: Write `AppTest` integration tests for the display logic with various `BenchmarkRun` data scenarios.
- [ ] Task 8: Manually test the display on the Streamlit page after a full benchmark run.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
- **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - Updated to Ready status after 4.8 completion
    - ...
