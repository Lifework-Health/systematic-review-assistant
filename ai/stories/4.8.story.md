# Story 4.8: Automated Metrics Calculation and Persistence

**Status:** Draft

## Goal & Context

**User Story:** As a System, I want after all items in a benchmark run are processed and `BenchmarkResultItem` records are created, that performance metrics (as per `docs/sr_metrics.md`) are automatically calculated by comparing `BenchmarkResultItem.final_decision` (SRA's output for the run) against `BenchmarkResultItem.human_decision` for all items in that run, and the corresponding `BenchmarkRun` record is updated by populating its individual typed metric columns.

**Context:** This story implements the automated calculation and storage of performance metrics, which is a core part of the benchmarking module. It follows the completion of item processing in US4.7 and populates the `BenchmarkRun` record defined in US4.3. This directly supports PRD FR5.1 and FR5.2 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   After all `BenchmarkResultItem` records for a specific `BenchmarkRun` have been created and stored (completion of US4.7 processing loop):
    1. Fetch all `BenchmarkResultItem` records associated with the current `BenchmarkRun` ID from the database.
    2. For each item, compare its `final_decision` (SRA's decision) with its `human_decision` (ground truth) to determine if it's a True Positive (TP), False Positive (FP), True Negative (TN), or False Negative (FN).
        - TP: `final_decision` is INCLUDE/True AND `human_decision` is True.
        - FP: `final_decision` is INCLUDE/True AND `human_decision` is False.
        - FN: `final_decision` is EXCLUDE/False AND `human_decision` is True.
        - TN: `final_decision` is EXCLUDE/False AND `human_decision` is False.
        - (Note: Handle `UNCERTAIN` in `final_decision` as incorrect if `human_decision` is `True` (FN) or `False` (FP). If `human_decision` is also `None`/`UNCERTAIN`, these items might be excluded from these specific metric calculations or handled as per a defined protocol for uncertain ground truth â€“ for MVP, assume `human_decision` is boolean and `final_decision` that is `UNCERTAIN` is treated as incorrect for the primary metrics like Sensitivity/Specificity derived from TP/FP/FN/TN).
    3. Aggregate these counts (TP, FP, FN, TN) for the entire benchmark run.
    4. Using these aggregate counts, calculate all performance metrics defined in `docs/sr_metrics.md`:
        - Sensitivity (Recall)
        - Specificity
        - Accuracy
        - PPV (Precision)
        - NPV
        - F1 Score
        - MCC (Matthews Correlation Coefficient)
        - Cohen's Kappa
        - PABAK (Prevalence and Bias Adjusted Kappa)
        - LR+ (Positive Likelihood Ratio)
        - LR- (Negative Likelihood Ratio)
    5. Update the `BenchmarkRun` record in the database by populating its dedicated, typed metric columns (e.g., `tp`, `fp`, `sensitivity`, `f1_score`, etc.) with these calculated values.
    6. This calculation and update process should occur within the same overall transaction as the creation of the `BenchmarkResultItem` records if feasible, or as a distinct, reliable step immediately following.

## Acceptance Criteria (ACs)

- AC1: After processing all items for a `BenchmarkRun`, the system correctly calculates the total counts of TP, FP, FN, and TN.
- AC2: All specified performance metrics (Sensitivity, Specificity, Accuracy, PPV, NPV, F1, MCC, Cohen's Kappa, PABAK, LR+, LR-) are correctly calculated based on the TP, FP, FN, TN counts and the formulas in `docs/sr_metrics.md`.
- AC3: The corresponding `BenchmarkRun` record in the database is updated with all the calculated metric values in their respective typed columns.
- AC4: Calculations correctly handle scenarios with zero denominators (e.g., if TP+FN = 0 for Sensitivity, result should be 0 or NaN/None as appropriate and stored as null in DB if the column is nullable float).

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Modify: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or wherever the benchmark execution logic from US4.7 resides, to add a call to the metrics calculation function).
    - Files to Create/Modify: A new utility module might be created for metrics calculations (e.g., `src/sr_assistant/benchmark/logic/metrics_calculator.py`) or logic can be added to an existing benchmark logic file.
    - Files to Interact With: `src/sr_assistant/core/models.py` (`BenchmarkRun`, `BenchmarkResultItem`), `src/sr_assistant/core/schemas.py` (`BenchmarkRunUpdate`).
    - Database interaction likely via repositories or direct session use.

-   **Key Technologies:**
    - Python 3.12
    - Pandas (potentially, for aggregating TP/FP/FN/TN from result items)
    - NumPy (potentially, for safe division or handling NaN)
    - SQLModel, SQLAlchemy
    - `scikit-learn.metrics` (optional, can be used for some calculations like `cohen_kappa_score`, `matthews_corrcoef`, `f1_score`, `precision_score`, `recall_score`, `accuracy_score` if it simplifies implementation, but ensure formulas align with `docs/sr_metrics.md`). Direct implementation of formulas is also acceptable.

-   **API Interactions / SDK Usage:**
    - Database interactions to fetch `BenchmarkResultItem` records for a run and to update the `BenchmarkRun` record.

-   **UI/UX Notes:**
    - Progress for this step might be indicated as "Calculating performance metrics..." after item processing finishes.

-   **Data Structures:**
    - List of `BenchmarkResultItem` objects/dictionaries.
    - `BenchmarkRunUpdate` Pydantic schema for updating the run with metrics.
    - Formulas from `docs/sr_metrics.md`.

-   **Environment Variables:**
    - Standard application environment variables for database connection.

-   **Coding Standards Notes:**
    - Ensure calculations are numerically stable (e.g., handling division by zero).
    - Functions for calculating individual metrics should be pure and testable where possible.

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - For each individual metric calculation function: Provide diverse sets of TP, FP, FN, TN values (including zeros, and cases leading to zero denominators) and assert that the calculated metric matches expected values (use `pytest.approx` for float comparisons).
    - Test the aggregation logic that counts TP, FP, FN, TN from a list of mock `BenchmarkResultItem` objects with various `final_decision` and `human_decision` combinations.
    - Test the overall function that takes a list of `BenchmarkResultItem` and returns a dictionary or `BenchmarkRunUpdate` object populated with all metrics.
-   **Integration Tests:**
    -   After US4.7 integration test runs and populates `BenchmarkResultItem` records (using mocked LLM outputs):
        - Trigger the metrics calculation and update process.
        - Fetch the updated `BenchmarkRun` record from the `sra_integration_test` DB.
        - Verify that all metric columns in the `BenchmarkRun` record are populated with values that are correctly calculated based on the underlying `BenchmarkResultItem` data.
-   **Manual/CLI Verification:**
    -   After a manual benchmark run (from US4.7) completes:
        - Inspect the `benchmark_runs` table in the Supabase-hosted `postgres` DB for the specific run.
        - Verify that all metric columns are populated.
        - Manually recalculate a few key metrics based on a sample of `benchmark_result_items` to cross-verify the stored values.

## Tasks / Subtasks

-   [ ] Task 1: Design and implement metric calculation functions.
    - [ ] Subtask 1.1: Create a function (e.g., in `src/sr_assistant/benchmark/logic/metrics_calculator.py`) that takes a list of `BenchmarkResultItem` (or relevant fields) for a run.
    - [ ] Subtask 1.2: Inside this function, calculate TP, FP, FN, TN counts.
    - [ ] Subtask 1.3: Implement separate (or nested) helper functions for each performance metric (Sensitivity, Specificity, F1, MCC, Kappa, etc.) using formulas from `docs/sr_metrics.md`. Ensure robust handling of division by zero (e.g., return 0.0 or None).
    - [ ] Subtask 1.4: The main function should return a dictionary or a `BenchmarkRunUpdate` object containing all calculated metric values.
-   [ ] Task 2: Integrate metrics calculation into the benchmark run workflow.
    - [ ] Subtask 2.1: In `human_benchmark_page.py` (or benchmark orchestration logic), after processing all items for a run, call the metrics calculation function.
    - [ ] Subtask 2.2: Fetch the current `BenchmarkRun` object.
    - [ ] Subtask 2.3: Use the calculated metrics to update the `BenchmarkRun` object.
    - [ ] Subtask 2.4: Persist the updated `BenchmarkRun` object to the database (e.g., using a repository update method).
-   [ ] Task 3: Write comprehensive unit tests for all metric calculation functions, covering edge cases (e.g., zero values for TP/FP/FN/TN).
-   [ ] Task 4: Write/update integration tests to verify that `BenchmarkRun` records are correctly updated with calculated metrics after a benchmark run.
-   [ ] Task 5: Manually test and verify metric calculations for a completed benchmark run.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
