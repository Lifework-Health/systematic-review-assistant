# Story 4.3: Define `BenchmarkRun` Model and Schemas

**Status:** Draft

## Goal & Context

**User Story:** As a Developer, I want to define the `BenchmarkRun` SQLModel and corresponding Pydantic schemas (`Base`, `Create`, `Update`, `Read`) so that benchmark execution details and all summary result metrics (as per `docs/sr_metrics.md` and PRD FR3.1) can be robustly persisted and accessed.

**Context:** This story establishes the database structure and data transfer objects for storing overall benchmark run information. It's a prerequisite for creating database migrations (US4.5) and for any logic that initiates, updates, or displays benchmark runs (US4.7, US4.9, US4.12). This directly supports PRD FR3.1 and FR3.3 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   Define a new SQLModel class `BenchmarkRun` in `src/sr_assistant/core/models.py`.
    -   It must include an `id` (UUID, primary key, default_factory=uuid.uuid4), `created_at` (AwareDatetime, default_factory=utcnow, nullable=False), and `updated_at` (AwareDatetime, default_factory=utcnow, sa_column_kwargs={"onupdate": utcnow}, nullable=False).
    -   It must have a `benchmark_review_id` (UUID, ForeignKey to `systematic_reviews.id`).
    -   It must include `config_details` (JSONB, nullable=True) to store configuration settings for the run (e.g., LLM models, prompt versions).
    -   It must include `run_notes` (TEXT, nullable=True) for user-provided notes.
    -   It MUST include individual, typed columns for all performance metrics listed in `docs/sr_metrics.md`. These columns should be nullable (e.g., `tp: int | None = Field(default=None)`, `sensitivity: float | None = Field(default=None)`).
        - Metrics: TP, FP, FN, TN, Sensitivity, Specificity, Accuracy, PPV (Precision), NPV, F1 Score, MCC (Matthews Correlation Coefficient), Cohen's Kappa, PABAK (Prevalence and Bias Adjusted Kappa), LR+ (Positive Likelihood Ratio), LR- (Negative Likelihood Ratio).
-   Define Pydantic schemas in `src/sr_assistant/core/schemas.py` for `BenchmarkRun`:
    - `BenchmarkRunBase(BaseSchema)`: Contains all fields common to create and read, including all metric fields (all optional or with defaults).
    - `BenchmarkRunCreate(BenchmarkRunBase)`: For creating new runs. `benchmark_review_id` should be mandatory. Metric fields are typically not set on creation.
    - `BenchmarkRunUpdate(BenchmarkRunBase)`: For updating runs, primarily to add calculated metrics. All fields should be optional.
    - `BenchmarkRunRead(BenchmarkRunBase)`: For returning run data, including `id`, `created_at`, `updated_at`.
-   All SQLModel fields and Pydantic schema fields MUST have field docstrings as per `docs/coding-standards.md` and `docs/data-models.md#Pydantic-Field-Documentation-Standard`.
-   Ensure all Pydantic schemas inherit from `core.schemas.BaseSchema`.
-   Update `docs/data-models.md` to include the new `BenchmarkRun` SQLModel and Pydantic schemas, including an ERD representation.

## Acceptance Criteria (ACs)

- AC1: `BenchmarkRun` SQLModel is defined in `src/sr_assistant/core/models.py` with all specified fields (`id`, `created_at`, `updated_at`, `benchmark_review_id`, `config_details`, `run_notes`) and individual nullable columns for all metrics from `docs/sr_metrics.md` (TP, FP, FN, TN, Sensitivity, Specificity, Accuracy, PPV, NPV, F1, MCC, Cohen's Kappa, PABAK, LR+, LR-).
- AC2: Pydantic schemas `BenchmarkRunBase`, `BenchmarkRunCreate`, `BenchmarkRunUpdate`, `BenchmarkRunRead` are defined in `src/sr_assistant/core/schemas.py`, inheriting from `BaseSchema` and accurately reflecting the `BenchmarkRun` model structure with appropriate optionality for each schema type.
- AC3: All fields in both the SQLModel and Pydantic schemas have clear field docstrings.
- AC4: `docs/data-models.md` is updated with the `BenchmarkRun` model and schema definitions and a revised ERD.
- AC5: Code passes all linter checks (Ruff, Pyright).

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Create/Modify: `src/sr_assistant/core/models.py` (add `BenchmarkRun` SQLModel)
    - File to Create/Modify: `src/sr_assistant/core/schemas.py` (add `BenchmarkRunBase`, `BenchmarkRunCreate`, `BenchmarkRunUpdate`, `BenchmarkRunRead` Pydantic schemas)
    - File to Modify: `docs/data-models.md` (add new model/schema definitions and update ERD)

-   **Key Technologies:**
    - Python 3.12
    - SQLModel
    - Pydantic
    - Mermaid (for ERD in `docs/data-models.md`)

-   **API Interactions / SDK Usage:**
    - Not applicable for this definition story.

-   **UI/UX Notes:**
    - Not applicable for this definition story.

-   **Data Structures:**
    -   **`BenchmarkRun` SQLModel:** Refer to Detailed Requirements for field specifications. Metric fields should be `int | None` for counts (TP, FP, FN, TN) and `float | None` for calculated ratios/coefficients.
        - `config_details`: Use `dict[str, Any]` for the Python type, mapping to `JSONB` in the database (via `sa_column=Column(JSONB)`, `default_factory=dict`).
    -   **Pydantic Schemas:** Structure as `Base`, `Create`, `Update`, `Read`.
        - `BenchmarkRunCreate`: `benchmark_review_id` is required. `config_details` can default to empty dict. `run_notes` optional. Metric fields should not be required for creation.
        - `BenchmarkRunUpdate`: All fields from `BenchmarkRunBase` should be optional to allow partial updates (especially for metrics).
        - `BenchmarkRunRead`: Include `id`, `created_at`, `updated_at` in addition to `BenchmarkRunBase` fields.

-   **Environment Variables:**
    - Not directly applicable for this definition story.

-   **Coding Standards Notes:**
    - Follow all standards in [`docs/coding-standards.md`](/docs/coding-standards.md).
    - Ensure `AwareDatetime` is used for timestamp fields in Pydantic schemas, and `datetime` with `timezone.utc` for SQLModel defaults and `onupdate` functions.
    - All SQLModel and Pydantic fields MUST have descriptive field docstrings.

## Testing Requirements

**Guidance:** Verify implementation against the ACs. For this story, testing is primarily through code review, linter checks, and successful generation of related artifacts (like DB migration in a subsequent story).

-   **Unit Tests:**
    - Not directly applicable for model/schema definitions themselves, but Pydantic schemas will be implicitly tested when used in other unit tests (e.g., service or repository tests that use these schemas for validation).
    - One could write simple unit tests to ensure Pydantic schemas can be instantiated correctly and validate example data if desired.
-   **Integration Tests:**
    - Not directly applicable, but successful database migration (US4.5) will validate the SQLModel.
-   **Manual/CLI Verification:**
    - Code review of `models.py` and `schemas.py` against requirements.
    - Review of the updated `docs/data-models.md` for accuracy and completeness.
    - Successful execution of Ruff and Pyright linters against the modified files.

## Tasks / Subtasks

-   [ ] Task 1: Define the `BenchmarkRun` SQLModel in `src/sr_assistant/core/models.py`.
    - [ ] Subtask 1.1: Add `id`, `benchmark_review_id`, `config_details`, `run_notes`, `created_at`, `updated_at` fields with correct types, defaults, and foreign keys.
    - [ ] Subtask 1.2: Add all metric fields (TP, FP, FN, TN, Sensitivity, Specificity, Accuracy, PPV, NPV, F1, MCC, Cohen's Kappa, PABAK, LR+, LR-) as nullable `int` or `float` `Field`s.
    - [ ] Subtask 1.3: Ensure correct table name (`benchmark_runs`) and add field docstrings.
-   [ ] Task 2: Define the Pydantic schemas (`BenchmarkRunBase`, `BenchmarkRunCreate`, `BenchmarkRunUpdate`, `BenchmarkRunRead`) in `src/sr_assistant/core/schemas.py`.
    - [ ] Subtask 2.1: Implement `BenchmarkRunBase` with all relevant fields (including metrics) as optional or with defaults.
    - [ ] Subtask 2.2: Implement `BenchmarkRunCreate` inheriting from Base, making `benchmark_review_id` mandatory.
    - [ ] Subtask 2.3: Implement `BenchmarkRunUpdate` inheriting from Base (all fields remain optional).
    - [ ] Subtask 2.4: Implement `BenchmarkRunRead` inheriting from Base, adding `id`, `created_at`, `updated_at`.
    - [ ] Subtask 2.5: Ensure all Pydantic schemas inherit from `core.schemas.BaseSchema` and have field docstrings.
-   [ ] Task 3: Update `docs/data-models.md`.
    - [ ] Subtask 3.1: Add the definition of the `BenchmarkRun` SQLModel.
    - [ ] Subtask 3.2: Add definitions for `BenchmarkRunBase`, `BenchmarkRunCreate`, `BenchmarkRunUpdate`, `BenchmarkRunRead` Pydantic schemas.
    - [ ] Subtask 3.3: Update the Mermaid ERD diagram to include `BenchmarkRun` and its relationship to `SystematicReview`.
-   [ ] Task 4: Run linters (Ruff, Pyright) and fix any issues in the new/modified code.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
