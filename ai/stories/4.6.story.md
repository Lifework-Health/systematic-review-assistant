# Story 4.6: Benchmark UI - Load and Display Protocol

**Status:** Draft

## Goal & Context

**User Story:** As a Benchmark User, I want a UI page (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`) that loads and displays the details of the seeded benchmark protocol so that I can understand the context (research question, criteria) of the benchmark before running it.

**Context:** This story creates the initial user interface for the benchmarking module. It depends on the benchmark protocol being seeded into the database (US4.1). It allows the user to view the protocol against which the benchmark will be run. This supports PRD FR4.1 and FR4.2 ([`docs/prd-benchmark-may.md`](/docs/prd-benchmark-may.md)).

## Detailed Requirements

-   Create a new Streamlit page, for example, `human_benchmark_page.py` within the `src/sr_assistant/benchmark/pages/` directory.
-   This page should be accessible from the main Streamlit application navigation (e.g., added to a list of pages in `main.py` or via Streamlit's native multi-page app capabilities if configured).
-   On loading, the page must fetch the specific benchmark `SystematicReview` record from the database. This record can be identified by its pre-defined `BENCHMARK_REVIEW_ID` (from `tools/seed_benchmark_data.py`).
-   The UI must display the following information from the fetched `SystematicReview` record:
    - Research Question (`research_question` field)
    - Background (`background` field, if available)
    - **PICO Elements**: Display the content of `criteria_framework_answers` (which contains plain text for population, intervention, comparison, outcome under keys like `population`, `intervention`, etc.). The XML tags that were used to construct `inclusion_criteria` should **not** be displayed here; show the plain text content for readability.
    - Full Inclusion Criteria (`inclusion_criteria` field): Display the XML-formatted string as stored in the database for user reference, perhaps in an expander or a less prominent way, as it's mainly for LLM consumption.
    - Full Exclusion Criteria (`exclusion_criteria` field, newline-delimited plain text).
-   If the benchmark protocol record cannot be found, display an appropriate error message (e.g., "Benchmark protocol not found. Please ensure data has been seeded.").

## Acceptance Criteria (ACs)

- AC1: A new Streamlit page for the benchmark module is created (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`) and is navigable.
- AC2: The page successfully fetches the benchmark `SystematicReview` record from the database using its known ID.
- AC3: The UI correctly displays the Research Question and Background from the benchmark protocol.
- AC4: The UI displays the PICO elements (Population, Intervention, Comparison, Outcome) from `criteria_framework_answers` in a readable, plain text format (XML tags stripped for display if they were present in the values, though the story for 4.1 now specifies plain text values in this JSONB).
- AC5: The UI displays the full `inclusion_criteria` (the XML-formatted string) and `exclusion_criteria` (the newline-delimited plain text string) from the benchmark protocol.
- AC6: An informative error message is shown if the benchmark protocol record is not found.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in [`docs/coding-standards.md`](/docs/coding-standards.md) and understand the project structure in [`docs/project-structure.md`](/docs/project-structure.md). Only story-specific details are included below.

-   **Relevant Files:**
    - File to Create: `src/sr_assistant/benchmark/pages/human_benchmark_page.py` (or similar name)
    - File to Modify (potentially): `src/sr_assistant/app/main.py` or equivalent for adding the new page to navigation.
    - Files to Read From (for reference): `src/sr_assistant/core/models.py` (for `SystematicReview`), `src/sr_assistant/core/schemas.py` (for `SystematicReviewRead`).
    - Interaction with: `src/sr_assistant/app/services.py` (specifically `ReviewService` to fetch the review).
    - `tools/seed_benchmark_data.py` (to know the `BENCHMARK_REVIEW_ID`).

-   **Key Technologies:**
    - Python 3.12
    - Streamlit
    - SQLModel (via `ReviewService`)

-   **API Interactions / SDK Usage:**
    - Use `ReviewService.get_review(review_id: BENCHMARK_REVIEW_ID)` to fetch the benchmark protocol.

-   **UI/UX Notes:**
    - Present the information clearly. Use `st.header`, `st.subheader`, `st.markdown`, `st.expander` as appropriate.
    - For PICO elements from `criteria_framework_answers`, display each component (Population, Intervention, etc.) with its text under a clear heading.
    - The raw XML `inclusion_criteria` might be best in an `st.expander` titled "Raw Inclusion Criteria (for LLM)".

-   **Data Structures:**
    - `SystematicReviewRead` Pydantic schema will be the primary data structure received from the `ReviewService`.
    - The `criteria_framework_answers` field (a dict) will be iterated to display PICO elements. The keys are `population`, `intervention`, `comparison`, `outcome`.

-   **Environment Variables:**
    - Standard application environment variables for database connection (used by `ReviewService`).

-   **Coding Standards Notes:**
    - Adhere to Streamlit best practices for page structure.
    - Use Loguru for any necessary logging.

## Testing Requirements

**Guidance:** Verify implementation against the ACs.

-   **Unit Tests:**
    - Not heavily applicable for a display-only Streamlit page if logic is minimal. If helper functions are created for formatting data for display, those should be unit-tested.
-   **Integration Tests (using `AppTest`):**
    - Test 1: Mock `ReviewService.get_review` to return a populated `SystematicReviewRead` object. Verify that all protocol details (RQ, background, PICO from `criteria_framework_answers` as plain text, XML `inclusion_criteria`, plain text `exclusion_criteria`) are correctly displayed in the UI elements (`st.markdown`, `st.text`, `st.expander`, etc.).
    - Test 2: Mock `ReviewService.get_review` to return `None`. Verify that the appropriate error message is displayed.
-   **Manual/CLI Verification:**
    - Run the Streamlit app (`uv run streamlit run src/sr_assistant/app/main.py`).
    - Navigate to the new benchmark page.
    - Verify that the seeded benchmark protocol (from US4.1) is displayed correctly and matches the data in the database.
    - Verify readability of PICO elements and the presentation of inclusion/exclusion criteria.

## Tasks / Subtasks

-   [ ] Task 1: Create the new Streamlit page file (e.g., `src/sr_assistant/benchmark/pages/human_benchmark_page.py`).
-   [ ] Task 2: Add the new page to the application's navigation structure.
-   [ ] Task 3: Implement logic to fetch the benchmark `SystematicReview` by its fixed ID using `ReviewService.get_review()`.
    - [ ] Subtask 3.1: Handle the case where the review is not found and display an error message.
-   [ ] Task 4: Implement UI elements to display the research question and background.
-   [ ] Task 5: Implement UI elements to display PICO elements from `criteria_framework_answers` in a readable, plain text format.
-   [ ] Task 6: Implement UI elements to display the full `inclusion_criteria` (XML string) and `exclusion_criteria` (newline-delimited plain text).
-   [ ] Task 7: Write `AppTest` integration tests to verify correct data display and error handling.
-   [ ] Task 8: Manually test the page by running the Streamlit application.

## Story Wrap Up (Agent Populates After Execution)

-   **Agent Model Used:** `<Agent Model Name/Version>`
-   **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
-   **Change Log:** {Track changes _within this specific story file_ if iterations occur}
    - Initial Draft
    - ...
